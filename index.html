

<!DOCTYPE html>
<html lang="en" dir="ltr">
  <head>
  </head>

  <div id="dog-image-container">

    <canvas style="touch-action: pinch-zoom;" id="canvas" width="1280" height="1280"></canvas>
    
    <canvas  style="touch-action: pinch-zoom;" hidden  id="offcanvas" width="1280" height="1280"></canvas>
    <canvas style="touch-action: pinch-zoom;" hidden  id="zffcanvas" width="1280" height="1280"></canvas>
    <!-- <script src="force.js"></script> -->

    <button id="enableMicBtn">Enable Microphone</button>
    <script>
      document.getElementById('enableMicBtn').addEventListener('click', async () => {
        await requestMicrophoneAccess();
        document.getElementById('enableMicBtn').style.display = "none"; // hide after use
      });
    </script>
    
<!-- Add this script tag anywhere in your HTML after the main module script -->
<script>
    // Global debug functions - accessible from console
    window.debugWebRTCConnections = function() {
      // console.log("=== WebRTC Debug ===");
      // console.log("PC exists:", !!pc);
      // console.log("Connection state:", pc?.connectionState);
      // console.log("ICE state:", pc?.iceConnectionState);
      // console.log("Signaling state:", pc?.signalingState);
      // console.log("Is polite:", isPolite);
      // console.log("Making offer:", makingOffer);
      // console.log("Ignore offer:", ignoreOffer);
      // console.log("WebRTC initialized:", webrtcInitialized);
      // console.log("Local senders:", pc?.getSenders()?.length || 0);
      // console.log("Remote receivers:", pc?.getReceivers()?.length || 0);
      
      // Check local audio tracks
      // console.log("--- LOCAL TRACKS ---");
      pc?.getSenders()?.forEach((sender, i) => {
        if (sender.track && sender.track.kind === 'audio') {
          // console.log(`Local audio ${i}:`, {
          //   enabled: sender.track.enabled,
          //   muted: sender.track.muted,
          //   readyState: sender.track.readyState,
          //   id: sender.track.id
          // });
        }
      });
      
      // Check remote audio tracks
      // console.log("--- REMOTE TRACKS ---");
      pc?.getReceivers()?.forEach((receiver, i) => {
        if (receiver.track && receiver.track.kind === 'audio') {
          // console.log(`Remote audio ${i}:`, {
          //   enabled: receiver.track.enabled,
          //   muted: false, //receiver.track.muted,
          //   readyState: receiver.track.readyState,
          //   id: receiver.track.id
          // });
        }
      });
      
      // Check audio elements on page
      const audioElements = document.querySelectorAll('audio');
      console.log("--- AUDIO ELEMENTS ---");
      console.log("Audio elements on page:", audioElements.length);
      audioElements.forEach((audio, i) => {
        // console.log(`Audio element ${i}:`, {
        //   src: audio.src,
        //   muted: audio.muted,
        //   volume: audio.volume,
        //   paused: audio.paused,
        //   autoplay: audio.autoplay
        // });
      });
    };
    
    window.forceOffer = function() {
      console.log("Forcing offer send...");
      if (signaling && signaling.sendOffer) {
        signaling.sendOffer();
      } else {
        // console.log("No signaling object available");
      }
    };
    
    window.togglePoliteness = function() {
      isPolite = !isPolite;
      // console.log("Toggled politeness, now:", isPolite ? "polite" : "impolite");
    };
    
    window.checkAudioContext = function() {
      // console.log("=== AUDIO CONTEXT DEBUG ===");
      // console.log("Global audio context exists:", !!globalAudioContext);
      // console.log("Audio context state:", globalAudioContext?.state);
      // console.log("Audio context ready:", audioContextReady);
      // console.log("Sample rate:", globalAudioContext?.sampleRate);
    };
    
    window.testMicAccess = function() {
      navigator.mediaDevices.getUserMedia({ audio: true })
        .then(stream => {
          // console.log("Microphone test successful");
          // console.log("Tracks:", stream.getTracks().map(t => ({
          //   kind: t.kind,
          //   enabled: t.enabled,
          //   muted: t.muted,
          //   readyState: t.readyState
          // })));
          stream.getTracks().forEach(track => track.stop());
        })
        .catch(err => {
          // console.error("Microphone test failed:", err);
        });
    };
    
    // Auto-run basic debug on page load
    setTimeout(() => {
      // console.log("WebRTC Debug functions loaded. Available commands:");
      // console.log("- debugWebRTCConnections()");
      // console.log("- forceOffer()"); 
      // console.log("- togglePoliteness()");
      // console.log("- checkAudioContext()");
      // console.log("- testMicAccess()");
    }, 2000);
    </script>
<!-- Add this script tag in your <head> section, before your main script -->
    <script>
        // Request microphone access immediately on page load
        let globalMicStream = null;
        
        async function requestMicrophoneAccess() {
    try {
        // Enforce HTTPS except on localhost
        if (location.protocol !== 'https:' && location.hostname !== 'localhost') {
            location.replace(`https://${location.hostname}${location.pathname}${location.search}`);
            return; // stop here, page will reload on HTTPS
        }

        // console.log("Requesting microphone access...");
        globalMicStream = await navigator.mediaDevices.getUserMedia({ audio: true });

        // console.log("Microphone access granted");
        
        // Optional: Show visual indicator that mic is ready
        document.body.style.border = "2px solid green";
        setTimeout(() => {
            if (document.body.style.border) {
                document.body.style.border = "";
            }
        }, 2000);

        // Make stream available globally
        window.getGlobalMicStream = () => globalMicStream;

    } catch (error) {
        console.error("Microphone access denied:", error);

        // Optional: Show visual indicator that mic was denied
        document.body.style.border = "2px solid red";
        setTimeout(() => {
            if (document.body.style.border) {
                document.body.style.border = "";
            }
        }, 3000);
    }
}

        
        // Request access as soon as possible
        // if (document.readyState === 'loading') {
        //     // document.addEventListener('DOMContentLoaded', requestMicrophoneAccess);
        // } else {
        //     // requestMicrophoneAccess();
        // }
        
        // Make the stream available globally for your WebRTC code
        window.getGlobalMicStream = () => globalMicStream;
        </script>

    <script type="module">


let pc = null;
let signaling = {};
let webrtcInitialized = false;
let pendingSignalingMessages = [];
let pendingIceCandidates = [];

// Politeness variables
let isPolite = false;
let makingOffer = false;
let ignoreOffer = false;

// AudioContext variables - MUST be here at top level
let globalAudioContext = null;
let audioContextReady = false;


let links = []
let hoverTarget = null;
let lock = 0
      import * as Tone from "https://cdn.skypack.dev/tone@14.8.49"; 
      const hexNotes = {
  "0": "C0",  "1": "C#2", "2": "D3",  "3": "D#2",
  "4": "E0",  "5": "F0",  "6": "F#2", "7": "G3",
  "8": "G#2", "9": "A3",  "A": "A#2", "B": "B0",
  "C": "C1",  "D": "C#1", "E": "D1",  "F": "D#1"
};
// --- Helpers ---

// Convert a Blob to AudioBuffer
async function blobToAudioBuffer(blob) {
  const arrayBuffer = await blob.arrayBuffer();
  return await new Promise((resolve, reject) => {
    const audioCtx = new (window.AudioContext || window.webkitAudioContext)();
    audioCtx.decodeAudioData(arrayBuffer, resolve, reject);
  });
}

// Merge two AudioBuffers into one
async function mergeBuffers(buffer1, buffer2) {
  const sampleRate = buffer1.sampleRate;
  const length = Math.max(buffer1.length, buffer2.length);
  const numChannels = Math.max(buffer1.numberOfChannels, buffer2.numberOfChannels);
  const offlineCtx = new OfflineAudioContext(numChannels, length, sampleRate);

  const source1 = offlineCtx.createBufferSource();
  source1.buffer = buffer1;
  source1.connect(offlineCtx.destination);
  source1.start();

  const source2 = offlineCtx.createBufferSource();
  source2.buffer = buffer2;
  source2.connect(offlineCtx.destination);
  source2.start();

  return await offlineCtx.startRendering();
}

// Convert AudioBuffer to WAV Blob
function audioBufferToWavBlob(audioBuffer) {
  const numChannels = audioBuffer.numberOfChannels;
  const sampleRate = audioBuffer.sampleRate;
  const length = audioBuffer.length * numChannels;
  const buffer = new ArrayBuffer(44 + length * 2);
  const view = new DataView(buffer);

  function writeString(view, offset, string) {
    for (let i = 0; i < string.length; i++) {
      view.setUint8(offset + i, string.charCodeAt(i));
    }
  }

  let offset = 0;
  writeString(view, offset, 'RIFF'); offset += 4;
  view.setUint32(offset, 36 + length * 2, true); offset += 4;
  writeString(view, offset, 'WAVE'); offset += 4;
  writeString(view, offset, 'fmt '); offset += 4;
  view.setUint32(offset, 16, true); offset += 4;
  view.setUint16(offset, 1, true); offset += 2;
  view.setUint16(offset, numChannels, true); offset += 2;
  view.setUint32(offset, sampleRate, true); offset += 4;
  view.setUint32(offset, sampleRate * numChannels * 2, true); offset += 4;
  view.setUint16(offset, numChannels * 2, true); offset += 2;
  view.setUint16(offset, 16, true); offset += 2;
  writeString(view, offset, 'data'); offset += 4;
  view.setUint32(offset, length * 2, true); offset += 4;

  for (let i = 0; i < audioBuffer.length; i++) {
    for (let ch = 0; ch < numChannels; ch++) {
      let sample = audioBuffer.getChannelData(ch)[i];
      sample = Math.max(-1, Math.min(1, sample));
      view.setInt16(offset, sample < 0 ? sample * 0x8000 : sample * 0x7FFF, true);
      offset += 2;
    }
  }

  return new Blob([view], { type: 'audio/wav' });
}



let rightClick = 0
        let dragTarget = {}

        let dragger = {}
        dragger.x = 0
        dragger.y = 0
        



// --- Generate Tone notes as AudioBuffer ---

// Create global audio context once at the top level
// const globalAudioContext = new (window.AudioContext || window.webkitAudioContext)();

// Create a single empty buffer to reuse
// let emptyBuffer = 



// let audioLock = 1 //turns off chimes
function visualizeAudioBuffer(audioBuffer, canvas, x = 0, y = 0, width = canvas.width, height = canvas.height) {
    const ctx = canvas.getContext("2d");
    const rawData = audioBuffer.getChannelData(0); // Use first channel
    const step = Math.ceil(rawData.length / width); // Samples per pixel

    // Clear the rectangle
    ctx.clearRect(x, y, width, height);

    // Draw background
    ctx.fillStyle = "#222";
    ctx.fillRect(x, y, width, height);

    // Draw waveform
    ctx.strokeStyle = "#0f0";
    ctx.lineWidth = 1;
    ctx.beginPath();

    for (let i = 0; i < width; i++) {
        const start = i * step;
        const end = Math.min(start + step, rawData.length);
        let min = 1.0;
        let max = -1.0;

        for (let j = start; j < end; j++) {
            const sample = rawData[j];
            if (sample < min) min = sample;
            if (sample > max) max = sample;
        }

        const y1 = y + ((1 + min) / 2) * height;
        const y2 = y + ((1 + max) / 2) * height;

        ctx.moveTo(x + i, y1);
        ctx.lineTo(x + i, y2);
    }

    ctx.stroke();
}

// Usage with your Tone.js snippet:// 4 seconds offline rendering (adjust to match your sequence length)




// --- Inline sendAudioObject ---

  
      const wsUrl = window.location.protocol.startsWith("http")
        ? `${window.location.protocol === "https:" ? "wss" : "ws"}://${window.location.host}`
        : "ws://localhost:8080"; // fallback for file://

      let ws = new WebSocket(wsUrl);

      const robust = makeRobustWebSocket(wsUrl, {heartbeatInterval: 20000, debug: true});

// expose minimal `ws` compatibility for the rest of your code:
ws = robust.raw; // NOTE: raw will be null initially; don't call ws.send() directly


// Message queue and processing system// Message queue and processing system
let messageQueue = [];
let processingMessages = false;
const MAX_MESSAGES_PER_BATCH = 5;
const PROCESSING_INTERVAL = 16; // ~60fps


window.onRobustOpen = async (rawSocket) => {
  console.log("WebSocket connected, initializing WebRTC...");

  try {
    determinePoleness();
    pc = await initWebRTC();
    console.log("PC created:", !!pc);

    const stream = await getMicStream();
    if (stream) {
      stream.getTracks().forEach(track => {
        pc.addTrack(track, stream);
      });
      console.log("Microphone added to peer connection");
    }

    signaling = setupSignaling(pc, robust);
    webrtcInitialized = true;

    setupConnectionRecovery();
    await processQueuedSignalingMessages();

    exposeToGlobal();
    console.log("WebRTC initialized successfully");

    if (!isPolite) {
      console.log("Impolite peer - sending initial offer");
      setTimeout(() => {
        if (signaling && signaling.sendOffer) signaling.sendOffer();
      }, 500);
    } else {
      console.log("Polite peer - waiting for remote offer");

      // üîë Safety net: if no offer comes, force renegotiation
      setTimeout(() => {
        if (pc && pc.signalingState === "stable" && webrtcInitialized) {
          console.warn("‚ö†Ô∏è Polite peer forcing renegotiation (no offer received)");
          signaling.sendOffer();
        }
      }, 3000);
    }

  } catch (error) {
    console.error("Failed to initialize WebRTC:", error);
    webrtcInitialized = false;
    exposeToGlobal();
    setTimeout(() => {
      window.onRobustOpen(rawSocket);
    }, 3000);
  }

  window.ws = rawSocket;
  window.updateGlobals?.(); // üîë run updateGlobals at the end if defined
};




async function initializeWebRTC() {
    // Reset state first
    await resetWebRTCState();

    // Create new peer connection
    pc = await initWebRTC();
    console.log("PC created:", !!pc);

    // Always reacquire a fresh mic stream after reset
    try {
        globalMicStream = await navigator.mediaDevices.getUserMedia({ audio: true });
        console.log("Fresh mic stream acquired after reset");
    } catch (err) {
        console.error("Mic request failed after reset:", err);
        globalMicStream = null;
    }

    // Attach mic tracks BEFORE signaling starts
    if (globalMicStream) {
  globalMicStream.getTracks().forEach(track => {
    console.log("Mic track state before attach:", {
      id: track.id,
      enabled: track.enabled,
      muted: track.muted,
      readyState: track.readyState
    });

    if (track.readyState === "live") {
      console.log("Attaching local mic track:", track.id);
      pc.addTrack(track, globalMicStream);
    } else {
      console.warn("Skipping stale mic track:", track.id, track.readyState);
    }
  });
  console.log("Microphone added to peer connection (checked)");
} else {
  console.warn("No microphone stream available to attach");
}


    // Force audio transceiver active
    pc.getTransceivers().forEach(t => {
        if (t.receiver.track.kind === "audio") {
            t.direction = "sendrecv";
        }
    });

    // Setup signaling AFTER tracks are added
    signaling = setupSignaling(pc, robust);
    webrtcInitialized = true;

    // Setup recovery handlers
    setupConnectionRecovery();

    // Process any queued messages
    await processQueuedSignalingMessages();

    exposeToGlobal();
    console.log("WebRTC initialized successfully");

    // Last fallback: if sender is muted after connection is established
    pc.addEventListener("connectionstatechange", async () => {
        if (pc.connectionState === "connected") {
            const sender = pc.getSenders().find(s => s.track && s.track.kind === "audio");
            if (sender && sender.track.muted) {
                console.warn("Local audio track is muted after connection ‚Äî replacing...");
                try {
                    const freshStream = await navigator.mediaDevices.getUserMedia({ audio: true });
                    await sender.replaceTrack(freshStream.getAudioTracks()[0]);
                    console.log("Replaced muted track with fresh one");
                } catch (err) {
                    console.error("Failed to replace muted track:", err);
                }
            }
        }
    });
}

function scheduleMessageProcessing() {
  if (processingMessages) return;
  
  processingMessages = true;
  requestIdleCallback(() => {
    processMessageQueue();
  }, { timeout: 50 }); // fallback timeout
}

async function processSignalingMessage(msg) {
    if (!pc) {
        console.error("No peer connection available");
        return;
    }

    if (pc.connectionState === "closed") {
        console.error("Peer connection is closed");
        return;
    }

    try {// --- inside processSignalingMessage ---
if (msg.offer) {
    console.log("Processing offer, current signaling state:", pc.signalingState);

    const offerCollision =
        (makingOffer || pc.signalingState !== "stable");

    ignoreOffer = !isPolite && offerCollision;

    if (ignoreOffer) {
        console.log("Ignoring offer due to collision (impolite peer)");
        return;
    }

    if (offerCollision && isPolite) {
        console.warn("‚ö†Ô∏è Glare detected (polite peer) ‚Üí rolling back");
        await pc.setLocalDescription({ type: "rollback" });
    }

    // ‚úÖ 1. Set remote description (offer)
    await pc.setRemoteDescription(new RTCSessionDescription(msg.offer));

    // ‚úÖ 2. Flush queued ICE after remote description
    if (pendingIceCandidates.length > 0) {
        console.log(`Flushing ${pendingIceCandidates.length} pending ICE candidates (after offer)`);
        for (const c of pendingIceCandidates) {
            try {
                await pc.addIceCandidate(new RTCIceCandidate(c));
            } catch (err) {
                console.error("Failed to add pending candidate:", err);
            }
        }
        pendingIceCandidates = [];
    }

    // Then create & send answer‚Ä¶
    const answer = await pc.createAnswer();
    await pc.setLocalDescription(answer);
    robust.send(JSON.stringify({ answer }));
    console.log("Answer sent successfully");
}

else if (msg.answer) {
    console.log("Processing answer, current state:", pc.signalingState);

    if (pc.signalingState === "have-local-offer") {
        // ‚úÖ 1. Set remote description (answer)
        await pc.setRemoteDescription(new RTCSessionDescription(msg.answer));

        // ‚úÖ 2. Flush queued ICE after remote description
        if (pendingIceCandidates.length > 0) {
            console.log(`Flushing ${pendingIceCandidates.length} pending ICE candidates (after answer)`);
            for (const c of pendingIceCandidates) {
                try {
                    await pc.addIceCandidate(new RTCIceCandidate(c));
                } catch (err) {
                    console.error("Failed to add pending candidate:", err);
                }
            }
            pendingIceCandidates = [];
        }
    } else {
        console.warn("Ignoring unexpected answer in state:", pc.signalingState);
    }
}

    } catch (error) {
        console.error("Error processing signaling message:", error);

        if (error.name === "InvalidStateError") {
            console.log("Invalid state error - resetting connection");
            await handleConnectionFailure();
        }
    } finally {
        makingOffer = false;
    }
}


async function processQueuedSignalingMessages() {
  console.log(`Processing ${pendingSignalingMessages.length} queued signaling messages`);
  
  while (pendingSignalingMessages.length > 0) {
    const msg = pendingSignalingMessages.shift();
    try {
      await processSignalingMessage(msg);
      // Small delay between messages to avoid overwhelming the connection
      await new Promise(resolve => setTimeout(resolve, 100));
    } catch (error) {
      console.error("Error processing queued signaling message:", error);
    }
  }
}


      // Robust WebSocket wrapper
function makeRobustWebSocket(wsUrl, opts = {}) {
  const {
    maxBackoff = 30000,     // max reconnection delay
    baseBackoff = 500,      // initial reconnection delay
    heartbeatInterval = 20000, // send ping every 20s (set 0 to disable)
    debug = true
  } = opts;

  let raw = null;
  let sendQueue = []; // Array of {data, isBinary}
  let ready = false;
  let closedByUser = false;
  let reconnectAttempts = 0;
  let heartbeatTimer = null;

  function log(...a){ if(debug) console.log('[robust-ws]', ...a); }

  function startHeartbeat() {
    stopHeartbeat();
    if (!heartbeatInterval) return;
    heartbeatTimer = setInterval(() => {
      if(raw && raw.readyState === WebSocket.OPEN){
        // lightweight ping text ‚Äî your server should reply or keepalive
        try { raw.send(JSON.stringify({type:'__ping__'})); }
        catch(e){ log('heartbeat send failed', e); }
      }
    }, heartbeatInterval);
  }
  function stopHeartbeat() {
    if (heartbeatTimer) { clearInterval(heartbeatTimer); heartbeatTimer = null; }
  }

  function createSocket() {
    raw = new WebSocket(wsUrl);
    raw.binaryType = "arraybuffer";
    ready = false;

    raw.addEventListener('open', () => { 
      log('open');
      reconnectAttempts = 0;
      ready = true;
      // flush queue
      while(sendQueue.length && raw.readyState === WebSocket.OPEN){
        const item = sendQueue.shift();
        try {
          raw.send(item.data);
        } catch (e) {
          log('error sending queued item, requeueing', e);
          sendQueue.unshift(item);
          break;
        }
      }
      startHeartbeat();

      // re-attach your existing socketize logic if needed by calling your handler
      if (typeof window.onRobustOpen === 'function') window.onRobustOpen(raw);
    });

    raw.addEventListener('message', (m) => {
        // console.log(m)
      // forward to existing handlers
      if (typeof window.onRobustMessage === 'function') {
        window.onRobustMessage(m, raw);
      } else {
        // fallback: call socketize message handler if it reads 'ws' variable
  
        
        



        log('message received - no onRobustMessage hook installed');
      }
    });

    raw.addEventListener('error', (e) => {
      log('socket error', e);
      // keep going ‚Äî 'close' will follow usually
    });

    raw.addEventListener('close', (ev) => {
      log('closed', ev.code, ev.reason);
      stopHeartbeat();
      ready = false;
      if (closedByUser) {
        log('closed by user ‚Äî not reconnecting');
        return;
      }
      // exponential backoff
      reconnectAttempts++;
      const backoff = Math.min(maxBackoff, baseBackoff * Math.pow(1.8, reconnectAttempts));
      log('reconnecting in', backoff, 'ms');
      setTimeout(() => {
        createSocket();
      }, backoff | 0);
    });
  }

  createSocket();

  return {
    send(data) {
      // Accept either ArrayBuffer/TypedArray/Blob or string
      if (!data) return;
      // If socket is open, attempt send synchronously and catch errors
      if (raw && raw.readyState === WebSocket.OPEN) {
        try {
          raw.send(data);
          return;
        } catch (e) {
          log('send failed, queueing', e);
          sendQueue.push({data});
        }
      } else {
        // queue until open
        sendQueue.push({data});
      }
    },
    close(code = 1000, reason) {
      closedByUser = true;
      stopHeartbeat();
      if (raw) raw.close(code, reason);
    },
    get raw() { return raw; },
    // useful for tests: inspect queue length
    get queueLength(){ return sendQueue.length; }
  };
}



 
      const synth = new Tone.Synth(); // DO NOT call .toDestination()
 


function audioBufferToPCM(audioBuffer) {
  const numChannels = audioBuffer.numberOfChannels;
  const length = audioBuffer.length * numChannels;
  const pcm = new Float32Array(length);

  for (let channel = 0; channel < numChannels; channel++) {
    const channelData = audioBuffer.getChannelData(channel);
    for (let i = 0; i < channelData.length; i++) {
      pcm[i * numChannels + channel] = channelData[i];
    }
  }

  return pcm.buffer; // ArrayBuffer
}
function prependBuffer(newBuffer, existingBuffer) {
  const newF32 = new Float32Array(newBuffer);
  const existingF32 = new Float32Array(existingBuffer);

  const combined = new Float32Array(newF32.length + existingF32.length);
  combined.set(newF32, 0);
  combined.set(existingF32, newF32.length);

  return combined.buffer; // ArrayBuffer
}


function arrayBufferToBlob(buffer, mimeType = "audio/wav") {
  return new Blob([buffer], { type: mimeType });
}


// const { TKGrid } = require("./astar_tk")

const squaretable = {}
for (let t = 0; t < 10000000; t++) {
    squaretable[`${t}`] = Math.sqrt(t)
    if (t > 999) {
        t += 9
    }
}
let highs = []

let made = 0
let seenIDs = new Set()
let movedMouse = 1
let startmouse = 50
let overlap = 50 //100
let fileon = false

let timespeed = 20
let timeon = 0
let addingOn = {}
let adding = 0

let allaud = []
let topnodes = []
let nodes = []
function getAudioFile() {
    return new Promise((resolve, reject) => {
        const input = document.createElement("input");
        input.type = "file";
        input.accept = "audio/*";
        input.style.display = "none"; // hide it
        document.body.appendChild(input);

        input.addEventListener("change", () => {
            const file = input.files[0];
            document.body.removeChild(input);
            if (file) {
                resolve(file);
            } else {
                reject(new Error("No file selected"));
            }
        });

        input.click(); // programmatically open file picker
    });
}


let uploaded = 0
let timerz = 0
let offset = {}
offset.x = 0
let video_recorder
let recording = 0
const gamepadAPI = {
    controller: {},
    turbo: true,
    connect: function (evt) {
        if (navigator.getGamepads()[0] != null) {
            gamepadAPI.controller = navigator.getGamepads()[0]
            gamepadAPI.turbo = true;
        } else if (navigator.getGamepads()[1] != null) {
            gamepadAPI.controller = navigator.getGamepads()[0]
            gamepadAPI.turbo = true;
        } else if (navigator.getGamepads()[2] != null) {
            gamepadAPI.controller = navigator.getGamepads()[0]
            gamepadAPI.turbo = true;
        } else if (navigator.getGamepads()[3] != null) {
            gamepadAPI.controller = navigator.getGamepads()[0]
            gamepadAPI.turbo = true;
        }
        for (let i = 0; i < gamepads.length; i++) {
            if (gamepads[i] === null) {
                continue;
            }
            if (!gamepads[i].connected) {
                continue;
            }
        }
    },
    disconnect: function (evt) {
        gamepadAPI.turbo = false;
        delete gamepadAPI.controller;
    },
    update: function () {
        gamepadAPI.controller = navigator.getGamepads()[0]
        gamepadAPI.buttonsCache = [];// clear the buttons cache
        for (var k = 0; k < gamepadAPI.buttonsStatus.length; k++) {// move the buttons status from the previous frame to the cache
            gamepadAPI.buttonsCache[k] = gamepadAPI.buttonsStatus[k];
        }
        gamepadAPI.buttonsStatus = [];// clear the buttons status
        var c = gamepadAPI.controller || {}; // get the gamepad object
        var pressed = [];
        if (c.buttons) {
            for (var b = 0, t = c.buttons.length; b < t; b++) {// loop through buttons and push the pressed ones to the array
                if (c.buttons[b].pressed) {
                    pressed.push(gamepadAPI.buttons[b]);
                }
            }
        }
        var axes = [];
        if (c.axes) {
            for (var a = 0, x = c.axes.length; a < x; a++) {// loop through axes and push their values to the array
                axes.push(c.axes[a].toFixed(2));
            }
        }
        gamepadAPI.axesStatus = axes;// assign received values
        gamepadAPI.buttonsStatus = pressed;
        // //console.log(pressed); // return buttons for debugging purposes
        return pressed;
    },
    buttonPressed: function (button, hold) {
        var newPress = false;
        for (var i = 0, s = gamepadAPI.buttonsStatus.length; i < s; i++) {// loop through pressed buttons
            if (gamepadAPI.buttonsStatus[i] == button) {// if we found the button we're looking for...
                newPress = true;// set the boolean variable to true
                if (!hold) {// if we want to check the single press
                    for (var j = 0, p = gamepadAPI.buttonsCache.length; j < p; j++) {// loop through the cached states from the previous frame
                        if (gamepadAPI.buttonsCache[j] == button) { // if the button was already pressed, ignore new press
                            newPress = false;
                        }
                    }
                }
            }
        }
        return newPress;
    },
    buttons: [
        'A', 'B', 'X', 'Y', 'LB', 'RB', 'Left-Trigger', 'Right-Trigger', 'Back', 'Start', 'Axis-Left', 'Axis-Right', 'DPad-Up', 'DPad-Down', 'DPad-Left', 'DPad-Right', "Power"
    ],
    buttonsCache: [],
    buttonsStatus: [],
    axesStatus: []
};
let canvas
let canvas_context
let keysPressed = {}
let FLEX_engine
let TIP_engine = {}
TIP_engine.x = 10000
TIP_engine.y = 10000
let XS_engine
let YS_engine
class Point {
    constructor(x, y) {
        this.x = x
        this.y = y
        this.radius = 0
    }
    pointDistance(point) {
        return (new LineOP(this, point, "transparent", 0)).hypotenuse()
    }
}
class LineOP {
    constructor(object, target, color, width) {
        this.object = object
        this.target = target
        this.color = color
        this.width = width
    }

    squareDistance() {
        let xdif = this.object.x - this.target.x
        let ydif = this.object.y - this.target.y
        return (xdif * xdif) + (ydif * ydif)
    }

    hypotenuse() {
        let xdif = this.object.x - this.target.x
        let ydif = this.object.y - this.target.y
        let hypotenuse = (xdif * xdif) + (ydif * ydif)
        if (hypotenuse < 10000000 - 1) {
            if (hypotenuse > 1000) {
                return squaretable[`${Math.round(10 * Math.round((hypotenuse * .1)))}`]
            } else {
                return squaretable[`${Math.round(hypotenuse)}`]
            }
        } else {
            return Math.sqrt(hypotenuse)
        }
    }

    angle() {
        return Math.atan2(this.object.y - this.target.y, this.object.x - this.target.x)
    }

    draw() {
        let linewidthstorage = canvas_context.lineWidth
        canvas_context.strokeStyle = this.color
        canvas_context.lineWidth = this.width
        canvas_context.beginPath()
        canvas_context.moveTo(this.object.x, this.object.y)
        canvas_context.lineTo(this.target.x, this.target.y)
        canvas_context.stroke()
        canvas_context.lineWidth = linewidthstorage
    }

    // --- new intersection method ---
    intersects(other) {
        function ccw(A, B, C) {
            return (C.y - A.y) * (B.x - A.x) > (B.y - A.y) * (C.x - A.x)
        }

        if(this.object == other.object || this.object == other.target || this.target == other.target || this.object == other.object){
            return false
        }
        const A = this.object
        const B = this.target
        const C = other.object
        const D = other.target

        // true if segments AB and CD intersect
        return (ccw(A, C, D) !== ccw(B, C, D)) && (ccw(A, B, C) !== ccw(A, B, D))
    }
}

class Rectangle {
    constructor(x, y, width, height, color, fill = 1, stroke = 0, strokeWidth = 1) {
        this.x = x
        this.y = y
        this.height = height
        this.width = width
        this.color = color
        this.xmom = 0
        this.ymom = 0
        this.stroke = stroke
        this.strokeWidth = strokeWidth
        this.fill = fill
    }
    draw() {
        canvas_context.fillStyle = this.color
        canvas_context.fillRect(this.x, this.y, this.width, this.height)
    }
    move() {
        this.x += this.xmom
        this.y += this.ymom
    }
    isPointInside(point) {
        if (point.x >= this.x) {
            if (point.y >= this.y) {
                if (point.x <= this.x + this.width) {
                    if (point.y <= this.y + this.height) {
                        return true
                    }
                }
            }
        }
        return false
    }
    doesPerimeterTouch(point) {
        if (point.x + point.radius >= this.x) {
            if (point.y + point.radius >= this.y) {
                if (point.x - point.radius <= this.x + this.width) {
                    if (point.y - point.radius <= this.y + this.height) {
                        return true
                    }
                }
            }
        }
        return false
    }
}


let rect1 = new Rectangle(0, 90, 1280, 100, "green")
class Circle {
    constructor(x, y, radius, color, xmom = 0, ymom = 0, friction = 1, reflect = 0, strokeWidth = 0, strokeColor = "transparent") {
        this.x = x
        this.y = y
        this.radius = radius
        this.color = color
        this.xmom = xmom
        this.ymom = ymom
        this.friction = friction
        this.reflect = reflect
        this.strokeWidth = strokeWidth
        this.strokeColor = strokeColor
        this.dragged = -1
    }
    draw(incoll = 1) {
        if (this.dragged == 1) {
            // console.log(2)
            this.x = TIP_engine.x
            this.y = TIP_engine.y
            if (!room.isPointInside(this)) {
                let l = new LineOP(this, room.center)
                let a = l.angle()
                this.x -= (l.hypotenuse() / 20) * Math.cos(a)
                this.y -= (l.hypotenuse() / 20) * Math.sin(a)
                if (room.isPointInside(this)) {
                    this.dragged *= -1
                }
            }
        }
        canvas_context.lineWidth = 2
        if(incoll == 1){
          canvas_context.strokeStyle = 'white'
        }else{

          canvas_context.strokeStyle = 'black'
        }
        canvas_context.beginPath();
        if (this.radius > 0) {
            canvas_context.arc(this.x, this.y, this.radius, 0, (Math.PI * 2), true)
            canvas_context.fillStyle = this.color
            canvas_context.fill()
            canvas_context.stroke();
        } else {
            //console.l\og("The circle is below a radius of 0, and has not been drawn. The circle is:", this)
        }
    }
    move() {
        if (this.reflect == 1) {
            if (this.x + this.radius > canvas.width) {
                if (this.xmom > 0) {
                    this.xmom *= -1
                }
            }
            if (this.y + this.radius > canvas.height) {
                if (this.ymom > 0) {
                    this.ymom *= -1
                }
            }
            if (this.x - this.radius < 0) {
                if (this.xmom < 0) {
                    this.xmom *= -1
                }
            }
            if (this.y - this.radius < 0) {
                if (this.ymom < 0) {
                    this.ymom *= -1
                }
            }
        }
        this.x += this.xmom
        this.y += this.ymom
    }
    unmove() {
        if (this.reflect == 1) {
            if (this.x + this.radius > canvas.width) {
                if (this.xmom > 0) {
                    this.xmom *= -1
                }
            }
            if (this.y + this.radius > canvas.height) {
                if (this.ymom > 0) {
                    this.ymom *= -1
                }
            }
            if (this.x - this.radius < 0) {
                if (this.xmom < 0) {
                    this.xmom *= -1
                }
            }
            if (this.y - this.radius < 0) {
                if (this.ymom < 0) {
                    this.ymom *= -1
                }
            }
        }
        this.x -= this.xmom
        this.y -= this.ymom
    }
    frictiveMove() {
        if (this.reflect == 1) {
            if (this.x + this.radius > canvas.width) {
                if (this.xmom > 0) {
                    this.xmom *= -1
                }
            }
            if (this.y + this.radius > canvas.height) {
                if (this.ymom > 0) {
                    this.ymom *= -1
                }
            }
            if (this.x - this.radius < 0) {
                if (this.xmom < 0) {
                    this.xmom *= -1
                }
            }
            if (this.y - this.radius < 0) {
                if (this.ymom < 0) {
                    this.ymom *= -1
                }
            }
        }
        this.x += this.xmom
        this.y += this.ymom
        this.xmom *= this.friction
        this.ymom *= this.friction
    }
    frictiveunMove() {
        if (this.reflect == 1) {
            if (this.x + this.radius > canvas.width) {
                if (this.xmom > 0) {
                    this.xmom *= -1
                }
            }
            if (this.y + this.radius > canvas.height) {
                if (this.ymom > 0) {
                    this.ymom *= -1
                }
            }
            if (this.x - this.radius < 0) {
                if (this.xmom < 0) {
                    this.xmom *= -1
                }
            }
            if (this.y - this.radius < 0) {
                if (this.ymom < 0) {
                    this.ymom *= -1
                }
            }
        }
        this.xmom /= this.friction
        this.ymom /= this.friction
        this.x -= this.xmom
        this.y -= this.ymom
    }
    isPointInside(point) {
        this.areaY = point.y - this.y
        this.areaX = point.x - this.x
        if (((this.areaX * this.areaX) + (this.areaY * this.areaY)) <= (this.radius * this.radius)) {
            return true
        }
        return false
    }
    doesPerimeterTouch(point) {
        this.areaY = point.y - this.y
        this.areaX = point.x - this.x
        if (((this.areaX * this.areaX) + (this.areaY * this.areaY)) <= ((this.radius + point.radius) * (this.radius + point.radius))) {
            return true
        }
        return false
    }
}
async function mainSetup() {
    pc = await initWebRTC();
    signaling = setupSignaling(pc, robust);
}



function setupSignaling(pc, ws) {
  async function sendOffer(force = false) {
    // Polite peers normally don‚Äôt initiate unless negotiationneeded or forced
    if (isPolite) {
  console.log("Polite peer - waiting for remote offer");
  setTimeout(() => {
    if (pc && pc.signalingState === "stable" && !pc.remoteDescription) {
      console.warn("‚ö†Ô∏è Timed out waiting for offer ‚Äî forcing negotiation");
      signaling.sendOffer(true); // force
    }
  }, 3000);
}


    try {
      makingOffer = true;
      // console.log("Creating offer...");

      const offer = await pc.createOffer({
        offerToReceiveAudio: true,
        offerToReceiveVideo: false,
      });

      // ‚úÖ Perfect negotiation glare check
      if (pc.signalingState !== "stable") {
        // console.warn("‚ö†Ô∏è Signaling state not stable, rolling back instead of sending offer");
        await pc.setLocalDescription({ type: "rollback" });
        return;
      }

      // console.log("Offer SDP:", offer.sdp);
      const audioCodecs = offer.sdp.match(/a=rtpmap:\d+ (\w+\/\d+)/g);
      // console.log("Available audio codecs:", audioCodecs);

      await pc.setLocalDescription(offer);
      ws.send(JSON.stringify({ offer }));
      // console.log("Offer sent successfully");
    } catch (error) {
      console.error("Failed to send offer:", error);
    } finally {
      makingOffer = false;
    }
  }

  async function sendAnswer(remoteOffer) {
    try {
      await pc.setRemoteDescription(new RTCSessionDescription(remoteOffer));
      const answer = await pc.createAnswer();

      // console.log("Answer SDP:", answer.sdp);

      await pc.setLocalDescription(answer);
      ws.send(JSON.stringify({ answer }));
      // console.log("Answer sent successfully");
    } catch (error) {
      // console.error("Failed to send answer:", error);
    }
  }

  return { sendOffer, sendAnswer };
}
function determinePoleness() {
    // Use WebSocket URL or a consistent identifier
    const urlHash = hashCode(window.location.href);
    isPolite = urlHash % 2 === 0;
    console.log("This peer is", isPolite ? "polite" : "impolite", "hash:", urlHash);
}

function hashCode(str) {
    let hash = 0;
    for (let i = 0; i < str.length; i++) {
        const char = str.charCodeAt(i);
        hash = ((hash << 5) - hash) + char;
        hash = hash & hash; // Convert to 32-bit integer
    }
    return Math.abs(hash);
}


async function testMicrophone() {
  try {
    const stream = window.getGlobalMicStream();
    if (!stream) {
      console.error("No microphone stream available");
      return;
    }
    
    console.log("Microphone tracks:", stream.getTracks().map(t => ({
      kind: t.kind,
      enabled: t.enabled,
      muted: t.muted,
      readyState: t.readyState
    })));
    
    // Create a local audio element to hear yourself (for testing only)
    const testAudio = document.createElement('audio');
    testAudio.srcObject = stream;
    testAudio.muted = false; // WARNING: This will cause feedback!
    testAudio.volume = 0.1; // Low volume to reduce feedback
    document.body.appendChild(testAudio);
    await testAudio.play();
    
    console.log("Playing microphone audio for 3 seconds (you should hear yourself)");
    setTimeout(() => {
      testAudio.remove();
      console.log("Microphone test complete");
    }, 3000);
    
  } catch (error) {
    console.error("Microphone test failed:", error);
  }
}




function debugWebRTCTracks() {
  if (!pc) {
    console.log("No peer connection");
    return;
  }
  
  console.log("Local tracks:");
  pc.getSenders().forEach((sender, i) => {
    if (sender.track) {
      console.log(`Sender ${i}:`, {
        kind: sender.track.kind,
        enabled: sender.track.enabled,
        muted: sender.track.muted,
        readyState: sender.track.readyState
      });
    }
  });
  
  console.log("Remote tracks:");
  pc.getReceivers().forEach((receiver, i) => {
    if (receiver.track) {
      console.log(`Receiver ${i}:`, {
        kind: receiver.track.kind,
        enabled: receiver.track.enabled,
        muted: false, //receiver.track.muted,
        readyState: receiver.track.readyState
      });
    }
  });
}




function setUp(canvas_pass, style = "#493939") {
    canvas = canvas_pass
    canvas_context = canvas.getContext('2d');
    canvas.style.background = style


// Run once at startup
mainSetup();

// Your 15 ms loop remains untouched
function mainLoop() {
    main()
    if(keysPressed['l']){
        console.log("Connection state:", pc.connectionState);
console.log("ICE state:", pc.iceConnectionState);
debugWebRTCTracks();
    }
}
setInterval(mainLoop, 15);







    document.addEventListener('keydown', (event) => {
        // if(event.key  == ' '){
        event.preventDefault()
        // }
        keysPressed[event.key] = true; //adds key to list of pressed
    });
    document.addEventListener('keyup', (event) => {
        delete keysPressed[event.key]; //for removing key from list of pressed
    });






    ///here
    let holdTarget = null;
    let holdTimeout = null;
    let didHold = false;
    const HOLD_DELAY = 250; // ms to count as a "hold"
    let stronko = ''

    let st1 = -1
    let st2 = 0
    window.addEventListener('contextmenu', e => {
        FLEX_engine = canvas.getBoundingClientRect();
        XS_engine = e.clientX - FLEX_engine.left;
        YS_engine = e.clientY - FLEX_engine.top;
        TIP_engine.x = XS_engine - offset.x;
        TIP_engine.y = YS_engine;
        TIP_engine.body = TIP_engine;
        e.preventDefault()


        let l = new LineOP(TIP_engine, TIP_engine);
        let min = 99999999;
        let index = -1;
        for (let t = 0; t < nodes.length; t++) {
            l.target = nodes[t].cap;
            let h = l.hypotenuse();
            if (h <= min && h <= nodes[t].offset.radius) {
                index = t;
                min = h;
            }
        }



        if (index > -1) {
        rightClick = 1
        dragTarget = nodes[index];

        dragger.x = TIP_engine.x
        dragger.y = TIP_engine.y
        dragger.xbolt = 0
        dragger.ybolt = 0
        }
        

    })



    window.addEventListener('pointerdown', e => {
        FLEX_engine = canvas.getBoundingClientRect();
        XS_engine = e.clientX - FLEX_engine.left;
        YS_engine = e.clientY - FLEX_engine.top;
        TIP_engine.x = XS_engine - offset.x;
        TIP_engine.y = YS_engine;
        TIP_engine.body = TIP_engine;

    if (e.button !== 0) return;
        if(rightClick == 1){
          return
        }
        rightClick = 0

        room.check(TIP_engine);
        // if (rect1.isPointInside(TIP_engine)) {

        //     if (fileon == false) {
        //         uploaded = 1
        //     } else {

        //         st1 = TIP_engine.x
        //     }
        // }
        let l = new LineOP(TIP_engine, TIP_engine);
        let min = 99999999;
        let index = -1;
        for (let t = 0; t < nodes.length; t++) {
            l.target = nodes[t].cap;
            let h = l.hypotenuse();
            if (h <= min && h <= nodes[t].offset.radius) {
                index = t;
                min = h;
            }
        }

        if (index > -1) {
            movedMouse = 1;

            // Reset all node audio
            for (let t = 0; t < nodes.length; t++) {
                nodes[t].content.message.volume = 0;
                nodes[t].content.message.pause();
                nodes[t].content.message.currentTime = 0;
            }

            holdTarget = nodes[index];
            didHold = false;

            // Start hold timer
            holdTimeout = setTimeout(() => {
                addingto(holdTarget); // immediate effect on hold
                startmouse = 50
overlap = 50;
                didHold = true; // mark that a hold occurred
            }, HOLD_DELAY);
        }
    });

    window.addEventListener('pointerup', async e => {

        FLEX_engine = canvas.getBoundingClientRect();
        XS_engine = e.clientX - FLEX_engine.left;
        YS_engine = e.clientY - FLEX_engine.top;
        TIP_engine.x = XS_engine - offset.x;
        TIP_engine.y = YS_engine;
        TIP_engine.body = TIP_engine;

        if(rightClick == 1){
          dragTarget = {}
          dragger.x =0 
          dragger.y =0 
          rightClick = 0
          return
        }
        if (e.button !== 0) return;


        if (rect1.isPointInside(TIP_engine)) {


            console.log(1)
            if (st1 > -1) {

                st2 = TIP_engine.x

                makeNodeFromClip(st1, st2, rect1, coloron, fileon)

            }
        }
        if (holdTimeout) {
            clearTimeout(holdTimeout);
            holdTimeout = null;

            if (didHold && holdTarget) {
                // AFTER HOLD ‚Üí run your full audio block
                // if (keysPressed[' ']) {
                startmouse = 50
overlap = 50;

                if (adding == 1) {
            




                  const audioResult = await stopRecording();
if (!audioResult || !audioResult.audioBlob) {
    console.error("sendAudioObject: missing audioBlob", audioResult);
    holdTarget = null;
    return;
}

const audioBlob = audioResult.audioBlob;
const url = URL.createObjectURL(audioBlob);
const audio = new Audio();
audio.src = url;
audio.addEventListener('error', e => {
    console.error('Audio loading error:', e);
    URL.revokeObjectURL(url);
});

let nodei = new Node(0, {
    message: {},
    x: holdTarget.cap.x + (Math.random() - 0.5),
    y: holdTarget.cap.y + 4
});
nodei.ID = holdTarget.ID + coloron + '.' + (childSearch(holdTarget.children, coloron) + 1);
seenIDs.add(nodei.ID);
seenIDs.add(holdTarget.ID);
// Merge noise + recording
const toneBuffer = await makenoise(coloron);                          // generate notes
const recordedBuffer = await blobToAudioBuffer(audioResult.audioBlob); // decode recording
const mergedBuffer = await mergeBuffers(toneBuffer, recordedBuffer);   // mix them
const mergedBlob = audioBufferToWavBlob(mergedBuffer);                 // convert to WAV

// Wrap merged blob in an Audio element
const mergedURL = URL.createObjectURL(mergedBlob);
const mergedAudio = new Audio(mergedURL);

// Assign node fields
nodei.content.message = mergedAudio;            // ‚úÖ now points to merged audio
nodei.messageType = "audio";
nodei.audioResult = { audioBlob: mergedBlob };  // keep blob for sending

// Send over socket
sendAudioObject(holdTarget.ID, { audioBlob: mergedBlob });

// optional debug
// visualizeAudioBuffer(mergedBuffer, canvas, 0, 0, 400, 400);
// lock = 1

                    allaud.push(audioResult);
                    holdTarget.children.push(nodei);
                    nodes.push(nodei);
                    console.log(nodei);

                    holdTarget = {};
                    addingOn = {};
                }
                // }
            } else if (holdTarget) {
                if (!didHold && holdTarget) {
                    movedMouse = 1;

                    // Reset all node audio
                    for (let t = 0; t < nodes.length; t++) {
                        nodes[t].content.message.volume = 0;
                        nodes[t].content.message.pause();
                        nodes[t].content.message.currentTime = 0;
                    }

                    const index = nodes.indexOf(holdTarget);

                    if (pausedex === index) {
                        // Node was already playing, pause it
                        holdTarget.content.message.pause();
                        holdTarget.content.message.currentTime = 0;
                        pausedex = -1;
                        if(keysPressed['c']){
                          holdTarget.collapsed *= -1
                          childrenhide(holdTarget)
                          // console.log(holdTarget)
                        }
                    } else {
                        // Play this node
                        if(keysPressed['c']){
                          holdTarget.collapsed *= -1
                          childrenhide(holdTarget)
                          // console.log(holdTarget)
                        }else{

                        holdTarget.content.message.volume = 1;
                        holdTarget.content.message.play();
                        holdTarget.touched = 1;
                        }
                    }

                    pausedex = index;
                    holdTarget = null;
                    addingOn = {};
                }

            }
        }
    });

    async function makeNodeFromClip(st1, st2, rect1, coloron, fileon) {
        if (st2 < st1) [st1, st2] = [st2, st1];

        let high = new Rectangle(st1, rect1.y, st2 - st1, rect1.height, coloron + '60');
        highs.push(high);

        console.log("pixel range:", st1, st2);

        // Decode to get duration
        const audioCtx = new (window.AudioContext || window.webkitAudioContext)();
        const arrayBuffer = await fileon.arrayBuffer();
        const decoded = await audioCtx.decodeAudioData(arrayBuffer);
        const duration = decoded.duration;

        let srat1 = (st1 / 1280) * duration;
        let srat2 = (st2 / 1280) * duration;

        if (srat2 <= srat1) {
            console.warn("Invalid slice", srat1, srat2);
            return;
        }

        // Clip audio -> returns a Blob
        let audioBlob = await clipAudio(fileon, srat1, srat2);

        // Wrap in an Audio element, like stopRecording does
        const url = URL.createObjectURL(audioBlob);
        const audio = new Audio();
        audio.src = url;
        audio.addEventListener("error", e => {
            console.error("Audio loading error:", e);
            URL.revokeObjectURL(url);
        });

        let nodei = new Node(0, {
            message: {},
            x: (st1 + st2) / 2,
            y: rect1.y + rect1.height
        });

        nodei.color = coloron;
        nodei.ID = Math.floor((st1 + st2) / 2);

        nodei.content.message = audio;
        nodei.messageType = "audio";
        nodei.audioResult = { audioBlob }; // keep original blob like stopRecording

        allaud.push(audio);
        topnodes.push(nodei);
        nodes.push(nodei);
        nodei.width = st2 - st1

        seenIDs.add(nodei.ID)
        // optional: send it over socket like your recording
        sendAudioElement(nodei.ID, audio, nodei.usercolor, nodei.body.x, nodei.body.y, st2 - st1);
    }




    window.addEventListener('pointermove', continued_stimuli);

    window.addEventListener('pointerup', e => {
        //for upclick actions
    })
    function continued_stimuli(e) {
        FLEX_engine = canvas.getBoundingClientRect();
        let ft = new Point(TIP_engine.x, TIP_engine.y)
        XS_engine = e.clientX - FLEX_engine.left;
        YS_engine = e.clientY - FLEX_engine.top;
        TIP_engine.x = XS_engine - offset.x
        TIP_engine.y = YS_engine
        TIP_engine.body = TIP_engine





        hovercheck = 10



        dragger.xbolt -= dragger.x - TIP_engine.x
        dragger.ybolt -= dragger.y - TIP_engine.y

        dragger.x = TIP_engine.x
        dragger.y = TIP_engine.y


        let l2 = new LineOP(ft, TIP_engine)
        if (l2.hypotenuse() > 1) {
            movedMouse = 1
        }
        if (l2.hypotenuse() > 10) {
            startmouse = 50
overlap = 50
        }
        if (l2.hypotenuse() > 50) {
            startmouse = 50
overlap = 50
        }
        //for moving mouse actions
    }
}

Number.prototype.between = function (a, b, inclusive) {
    var min = Math.min(a, b),
        max = Math.max(a, b);
    return inclusive ? this >= min && this <= max : this > min && this < max;
}

let setup_canvas = document.getElementById('canvas') //getting canvas from document
let offcanvas = document.getElementById('offcanvas') //getting canvas from document
let zffcanvas = document.getElementById('zffcanvas') //getting canvas from document

let off_context = offcanvas.getContext('2d');
let zff_context = zffcanvas.getContext('2d');
setUp(setup_canvas) // setting up canvas refrences, starting timer. 

function getRandomColor() { // random color
    var letters = '0123456789ABCDEF';
    var color = '#';
    for (var i = 0; i < 6; i++) {
        color += letters[(Math.floor(Math.random() * 16) + 0)];
    }
    return color;
}
let coloron = getRandomColor()
let worldcolor = 'olive'
let nodeid = 0
class Node {
    constructor(type, content) {
      this.collapsed = -1
        this.sending = 1
        this.heading = 0
        this.width = -1
        this.linker = new LineOP(new Point(0,0),new Point(0,0))
        this.usercolor = coloron
        this.touched = 0
        this.ID = nodeid
        seenIDs.add(nodeid)
        // nodeid++
        this.body = {}
        this.body.type = type
        this.content = content
        this.body.x = content.x
        this.body.y = content.y
        this.offset = {}
        this.offset.x = 0
        this.offset.friction = .8
        this.offset.xmom = 0
        this.offset.y = 0
        this.offset.radius = 12
        this.children = []
        this.type = type
        this.l = new LineOP(this.body, this.body)
        this.cap = {}
        this.cap.x = this.body.x
        this.cap.y = this.body.y
        this.layer = 0
        this.parent = {}
        this.offset.colorball = worldcolor
        this.latentColor = getRandomColor()
        this.unik = 1 //Math.random() * 3

        this.childing = 0
        // this.parent.cap = new Point(this.body.x+1, this.body.y+1)
    }
    dragchild(){

      for (let t = 0; t < this.children.length; t++) {
        this.children[t].body.x += dragger.xbolt
          this.children[t].body.y += dragger.ybolt
          this.children[t].dragchild()
          }
    }
    offsetting() {
        if(this == dragTarget){
          this.body.x += dragger.xbolt
          this.body.y += dragger.ybolt
          
            this.dragchild()
            dragger.xbolt = 0
            dragger.ybolt = 0
        }
        if (topnodes.includes(this)) {
            return
        }




        this.hash = {}
        for (let t = 0; t < nodes.length; t++) {
            if (this != nodes[t]) {
                this.l.object = this.cap
                this.l.target = nodes[t].cap

                this.hash[t] = {}
                this.hash[t].distance = Math.max(this.l.hypotenuse() + 1, 1)
                this.hash[t].x = nodes[t].cap.x
                this.hash[t].y = nodes[t].cap.y
                this.hash[t].radius = nodes[t].offset.radius
                this.hash[t].p = nodes.indexOf(nodes[t])
            }
        }

        let keys = Object.keys(this.hash)
        let force = {}
        let force2 = {}

        if(this.heading > -99999999 && this.heading != 0){
            this.offset.xmom -= ((this.heading*(this.offset.radius+9))*2)/10000
            this.offset.xmom = Math.sign(this.offset.xmom)*Math.min(Math.abs(this.offset.xmom), this.offset.radius*2)
            // this.heading =0
        }


        for (let t = 0; t < keys.length; t++) {
            force.x = 0
            force.y = 0
            for(let w= 0;w<15;w++){
                
            if (this.hash[keys[t]].distance < (this.offset.radius + this.hash[keys[t]].radius)+9) {

                force.x += Math.sign(this.hash[keys[t]].x - (this.body.x + this.offset.x)) / this.hash[keys[t]].distance
                // force.y += Math.sign(this.hash[keys[t]].y - (this.body.y + this.offset.y)) / this.hash[keys[t]].distance


                if (this.childos != 1) {
                    this.offset.x -= force.x / 1.5
                    // this.offset.y -= force.y / 1
                } else {

                    this.offset.x -= force.x / 9
                    // this.offset.y -= force.y / 1.6
                }
            } else {
                if (this.hash[keys[t]].distance < (this.offset.radius + this.hash[keys[t]].radius) * 1.1) {
                    if (this.hash[keys[t]].distance > (this.offset.radius + this.hash[keys[t]].radius) * .1) {


                        force.x += Math.sign(this.hash[keys[t]].x - (this.body.x + this.offset.x)) / this.hash[keys[t]].distance
                        // force.y  += (this.hash[keys[t]].y-(this.body.y+this.offset.y))/this.hash[keys[t]].distance


                        if (this.childos != 1) {
                            this.offset.x += force.x / 300
                            // this.offset.y += force.y / 50
                        } else {

                            this.offset.x += force.x / 1500
                            // this.offset.y += force.y / 550
                        }
                    }
                }
            }
            }
        }

        if (!(topnodes.includes(this))) {
            for (let t = 0; t < keys.length; t++) {
                if (this.hash[keys[t]].p == nodes.indexOf(this.parent)) {

                    force2.x = 0
                    force2.y = 0
                    if (this.hash[keys[t]].distance > this.offset.radius * 2) {

                        force2.x -= (this.hash[keys[t]].x - (this.body.x + this.offset.x)) / this.hash[keys[t]].distance
                        force2.y -= (this.hash[keys[t]].y - (this.body.y + this.offset.y)) / this.hash[keys[t]].distance


                        if (this.childos != 1) {
                            this.offset.x -= force2.x / 12
                            // this.offset.y -= force2.y / 2
                        } else {

                            this.offset.x -= force2.x / 300
                            // this.offset.y -= force2.y / 55
                        }
                    }
                }
            }
        }
        // console.log(this.hash)

        if (this.touched == 0) {

            // this.offset.x -= (Math.random()-.5)*1
            // this.offset.y -=  (Math.random()-.5)*1

        }
        // if(topnodes.includes(this)){
        let l = (this.cap.y - this.parent.cap.y)


        if (this.childos != 1) {

            if (l < Math.min(Math.max(this.offset.radius / 1.4, 12), 30) * (1.5 + this.unik)) {

                this.offset.y += .9 * Math.sqrt(this.layer)
            } else {
                this.offset.y -= .2 * Math.sqrt(this.layer)

            }
        } else {
            if (l < Math.min(Math.max(this.offset.radius / 1.4, 12), 30) * (1.5 + this.unik)) {

                this.offset.y += 0 * Math.sqrt(this.layer)
            } else {
                this.offset.y -= 0 * Math.sqrt(this.layer)

            }

        }
        // if(l.hypotenuse() > 60){

        //     this.offset.x -= (this.cap.x - this.parent.cap.x)/20
        //     this.offset.y -= (this.cap.y - this.parent.cap.y)/20
        // }

        for (let t = 0; t < this.children.length; t++) {
            let l = this.children[t].cap.y - this.cap.y
            if (l < Math.min(Math.max(this.offset.radius / 1.4, 12), 30) * (1.5 + this.unik)) {

                this.children[t].offset.y += .9 * Math.sqrt(this.children[t].layer)
            } else {

                this.children[t].offset.y -= .2 * Math.sqrt(this.children[t].layer)
            }
            //     if(l.hypotenuse() > 60){

            //         this.children[t].offset.x -= (this.children[t].cap.x - this.cap.x)/20
            //         this.children[t].offset.y -= (this.children[t].cap.y - this.cap.y)/20
            //     }
        }

        // }else{

        // }

        // let l = new LineOP(new Point(0,0), new Point(1000,1000), "blue",1)
        // l.object = this.body
        for (let t = 0; t < this.children.length; t++) {
            // this.children[t].offset.x += (Math.sign((this.body.x+this.offset.x) - (this.children[t].body.x-this.children[t].offset.x)))*2
            // this.children[t].offset.y += (Math.sign((this.body.y+this.offset.y) - (this.children[t].body.y-this.children[t].offset.y)))*2
            // l.target = new Point(this.children[t].offset.x+this.children[t].body.x, this.children[t].offset.y+this.children[t].body.y)
            // l.draw()
        }

        let radsnap = new LineOP(TIP_engine, this.cap)

        this.offset.x += this.offset.xmom
        this.offset.xmom *= this.offset.friction
        this.checker = new Circle(this.cap.x, this.cap.y, this.offset.radius, 'red')


        if (this.checker.isPointInside(TIP_engine)) {
            for (let t = 0; t < nodes.length; t++) {
                nodes[t].childing = 0
                childrenset(nodes[t])
            }
            this.childos = 0 //1 for lock mouse
            this.childing = 1
            this.offset.colorball = this.latentColor
            childrenset(this)
            made = 1
        } else {
            if (made <= -2) {
                if (this.childos == 1) {
                    this.offset.x -= (this.cap.x - TIP_engine.x) / 4
                    this.offset.y -= (this.cap.y - TIP_engine.y) / 4
                }
                this.childos = 0
                this.childing = 0
                // this.offset.colorball = worldcolor
                childrenset(this)

            }

        }
        this.offset.radius *= 14
        // this.offset.radius = 12
        this.offset.radius += Math.max(30 + (10 - (radsnap.hypotenuse() / 1)) / 10, 7)
        if (this.childing == 1) {
            this.offset.radius += 14
            this.offset.radius /= 15
        } else {

            this.offset.radius /= 15
        }
//something like this

        // this.offset.x+= this.children.length
    }
}

function childrenset(node) {
    for (let t = 0; t < node.children.length; t++) {
        // console.log('s')
        node.children[t].offset.colorball = node.offset.colorball
        node.children[t].childing = node.childing
        childrenset(node.children[t])
    }
}
function childrenhide(node) {
    for (let t = 0; t < node.children.length; t++) {
        // console.log('s')
        node.children[t].collapsed = node.collapsed
        childrenhide(node.children[t])
    }
}
let circle = new Circle(0, 0, 1, worldcolor)

function drawNode(node) { 
    // node.body.x -= .1
    if (node.type == 0) { //audio react
        circle.x = node.body.x + node.offset.x
        circle.y = node.body.y + node.offset.y
        circle.radius = node.offset.radius
        circle.color = node.offset.colorball
        // if(keysPressed['g']){

        if (node == addingOn) {
            circle.color = '#ffffff'
        } else if (node == dragTarget) {
            circle.color = '#444944'
        } else {
            circle.color = node.usercolor + (node.touched == 0 ? '' : 'a0')
        }
        circle.draw(node.collapsed)
        // }

        if (keysPressed['f']) {
            let r = new Circle(circle.x + 50, circle.y + 50, circle.radius, node.latentColor)
            r.draw()
        }
        node.cap = {}
        node.cap.x = circle.x
        node.cap.y = circle.y
        let str = Math.round(node.content.message.duration*1000)
        canvas_context.strokeStyle ="black"
        canvas_context.fillStyle ="white"
        canvas_context.font = `${Math.round(node.offset.radius/1.8)}px arial`
        canvas_context.strokeText(str, circle.x-(node.offset.radius/1.8), circle.y+(node.offset.radius/10))
        canvas_context.fillText(str, circle.x-(node.offset.radius/1.8), circle.y+(node.offset.radius/10))

        for (let t = 0; t < node.children.length; t++) {
            node.children[t].layer = node.layer + 1
            node.children[t].parent = node
     
            if(node.collapsed == -1){
            drawNode(node.children[t])
            }else{
              drawNodelight(node.children[t])
            }
            // let link = new LineOP(node.cap, node.children[t].cap, "blue", 2)
            // link.draw()
        }
    } else {

    }
}

function drawNodelight(node) { 
    // node.body.x -= .1
    if (node.type == 0) { //audio react
        circle.x = node.body.x + node.offset.x
        circle.y = node.body.y + node.offset.y
        circle.radius = node.offset.radius
        circle.color = node.offset.colorball
        // if(keysPressed['g']){

        if (node == addingOn) {
            circle.color = '#ffffff'
        } else if (node == dragTarget) {
            circle.color = '#444944'
        } else {
            circle.color = node.usercolor + (node.touched == 0 ? '' : 'a0')
        }
        // circle.draw()
        // }

        if (keysPressed['f']) {
            let r = new Circle(circle.x + 50, circle.y + 50, circle.radius, node.latentColor)
            r.draw()
        }
        node.cap = {}
        node.cap.x = circle.x
        node.cap.y = circle.y
        let str = node.content.message
        for (let t = 0; t < node.children.length; t++) {
            node.children[t].layer = node.layer + 1
            node.children[t].parent = node
            if(node.collapsed == -1){
            drawNode(node.children[t])
            }else{
              drawNodelight(node.children[t])
            }
            // let link = new LineOP(node.cap, node.children[t].cap, "blue", 2)
            // link.draw()
        }
    } else {

    }
}

function drawNodeD(node) {
    if (node.type == 0) {
        for (let t = 0; t < node.children.length; t++) {
            let link = new LineOP(
                node.cap,
                node.children[t].cap,
                "white", 
                node.offset.radius / 4
            )
            node.children[t].linker = link
            link.secret = node
            if(node.collapsed == -1){
              link.draw()
            links.push(link)
              drawNodeD(node.children[t])
            }
        }

    }

}

function setupConnectionRecovery() {
    if (!pc) return;
    
    pc.onconnectionstatechange = async () => {
    console.log("WebRTC connection state:", pc.connectionState);
    
    switch (pc.connectionState) {
        case 'failed':
        case 'disconnected':
            console.log("Connection failed/disconnected, attempting recovery...");
            await handleConnectionFailure();
            break;
        case 'connected':
            console.log("WebRTC connection established!");
            
            // Auto-start timeline when connected
            setTimeout(() => {
                if (globalAudioContext && !audioTimeline) {
                    initializeAudioTimeline();
                }
            }, 1000);
            break;
    }
};
    pc.oniceconnectionstatechange = async () => {
        console.log("ICE connection state:", pc.iceConnectionState);
        
        if (pc.iceConnectionState === 'failed') {
            console.log("ICE connection failed, restarting...");
            await restartIce();
        }
    };
}
async function handleConnectionFailure() {
    // Wait a bit before trying to recover
    await new Promise(resolve => setTimeout(resolve, 2000));

    if (!pc || pc.connectionState === 'failed' || pc.connectionState === 'closed') {
        console.log("Re-initializing WebRTC after failure/refresh...");
        await resetWebRTCState();
        await initializeWebRTC();

        // Impolite peer should send offer after reset
        if (!isPolite && signaling && signaling.sendOffer) {
            setTimeout(() => {
                console.log("Impolite peer forcing new offer after reset");
                signaling.sendOffer();
            }, 1000);
        }
    }
}

async function restartIce() {
    if (pc && pc.connectionState !== 'closed') {
        try {
            await pc.restartIce();
        } catch (error) {
            console.error("ICE restart failed:", error);
            await handleConnectionFailure();
        }
    }
}
async function resetWebRTCState() {
  console.log("Resetting WebRTC state...");

  // Close and cleanup old peer connection
  if (pc) {
    try {
      pc.close();
    } catch (e) {
      console.warn("Error closing old PC:", e);
    }
  }
  pc = null;
  signaling = null;
  webrtcInitialized = false;

  // Clear any queued messages
  if (window.pendingSignalingMessages) {
    window.pendingSignalingMessages = [];
  }

  // üîë Cleanup remote audio element to prevent duplicates
  const audioEl = document.getElementById("remoteAudio");
  if (audioEl) {
    console.log("Cleaning up old remote audio element");
    audioEl.srcObject = null;
    // Optional: remove element completely if you want a fresh one next time
    // audioEl.remove();
  }

  console.log("WebRTC state reset complete");
}



function drawNodeL(node) {
    let lout = []

    let valid = false
    for (let u = 0; u < links.length; u++) {
        // console.log(links)
        if (node.linker.intersects(links[u]) && nodes[u]!=node){
            console.log('sss')
            valid = true
            if(topnodes.includes(node) || topnodes.includes(links[u].secret)){


            }else{

            let x1 =  links[u].secret.body.x+links[u].secret.offset.x
            let x2 =  node.body.x+node.offset.x
            let y1 =  links[u].secret.body.y+links[u].secret.offset.y
            let y2 =  node.body.y+node.offset.y


            // node.body.y = y1
            // links[u].secret.body.y = y2
            node.body.x = x1
            links[u].secret.body.x = x2

// reset offsets
node.offset.x = 0;
// node.offset.y = 0;
links[u].secret.offset.x = 0;
// links[u].secret.offset.y = 0;

for (let t = 0; t < node.children.length; t++) {
    node.children[t].offset.x -= (x2-x1)/1
}
for (let t = 0; t < links[u].secret.children.length; t++) {
    links[u].secret.children[t].offset.x += (x2-x1)/1
}

            }

        }
    }



        for (let t = 0; t < node.children.length; t++) {

            // check if links[t] intersects with any other link
           

            // if(valid){
            //     node.children[t].offset.x -= (node.linker.object.x - node.linker.target.x)/3
            //     node.offset.x -=(node.linker.object.x - node.linker.target.x)/3

                
            // for (let u = 0; u < lout.length; u++) {

            //     nodes[lout[u]].offset.x +=(node.linker.object.x - node.linker.target.x)/3

            // }
            // }
            
            // drawNodeL(node.children[t])
        }
}



let aud = new Audio()
allaud.push(aud)
aud.src = 'src.wav'
for (let t = 0; t < 1; t++) { //basenode
    let nodei = new Node(0, { 'message': aud, 'x': 640, 'y': 640 })
    nodei.color = `rgb(${t * 100}, ${0 * 100},${0 * 100})`
    allaud.push(nodei.content.message)

    topnodes.push(nodei)
    nodes.push(nodei)
    // for(let k = 0;k<2;k++){
    //     let n1 = new Node(0, {'message':'child1', 'x':500+Math.random() ,'y': 200+k} )
    // n1.color = `rgb(${t*100}, ${k*100},${0*100})`
    // nodei.children.push(n1)
    //     nodes.push(n1)

    // for(let j = 0;j<2;j++){
    //     let n3 = new Node(0, {'message':'child2', 'x':500+Math.random() ,'y':200+j})
    // n3.color = `rgb(${t*100}, ${k*100},${j*100})`
    //     n1.children.push(n3)
    //     nodes.push(n3)

    //     for(let r = 0;r<2;r++){
    //         let n4 = new Node(0, {'message':'child2', 'x':500+Math.random() ,'y':200+j})
    //         n4.color = `rgb(${r*100}, ${k*100},${j*100})`
    //         n3.children.push(n4)
    //         nodes.push(n4)



    //     for(let q = 0;q<2;q++){
    //         let n5 = new Node(0, {'message':'child2', 'x':500+Math.random() ,'y':200+j})
    //         n5.color = `rgb(${q*100}, ${k*100},${j*100})`
    //         n4.children.push(n5)
    //         nodes.push(n5)
    //     }

    //     }



    // }


    // }
}


// for(let t =0 ;t<1;t++){
//     let nodei = new Node(0, {'message':'top', 'x':800+Math.random(),'y':200})
//     nodei.color = `rgb(${t*100}, ${0*100},${0*100})`
//     topnodes.push(nodei)
//     nodes.push(nodei)
//     for(let k = 0;k<1;k++){
//         let n1 = new Node(0, {'message':'child1', 'x':800+Math.random() ,'y': 200+k} )
//     n1.color = `rgb(${t*100}, ${k*100},${0*100})`
//     nodei.children.push(n1)
//         nodes.push(n1)

//     for(let j = 0;j<1;j++){
//         let n3 = new Node(0, {'message':'child2', 'x':800+ (j),'y':200+j})
//     n3.color = `rgb(${t*100}, ${k*100},${j*100})`
//         n1.children.push(n3)
//         nodes.push(n3)

//         for(let r = 0;r<2;r++){
//             let n4 = new Node(0, {'message':'child2', 'x':800+ (r-2) ,'y':200+j})
//             n4.color = `rgb(${r*100}, ${k*100},${j*100})`
//             n3.children.push(n4)
//             nodes.push(n4)



//         for(let q = 0;q<3;q++){
//             let n5 = new Node(0, {'message':'child2', 'x':800+ (q-2) ,'y':200+j})
//             n5.color = `rgb(${q*100}, ${k*100},${j*100})`
//             n4.children.push(n5)
//             nodes.push(n5)
//         }

//         }



//     }


//     }
// }
function pointInPolygon(point, polygon) {
    let inside = false;
    for (let i = 0, j = polygon.length - 1; i < polygon.length; j = i++) {
        const xi = polygon[i].x, yi = polygon[i].y;
        const xj = polygon[j].x, yj = polygon[j].y;

        const intersect = ((yi > point.y) !== (yj > point.y)) &&
            (point.x < (xj - xi) * (point.y - yi) / (yj - yi) + xi);

        if (intersect) inside = !inside;
    }
    return inside;
}

class RoomShape {
    constructor(sides) {

        this.dots = []
        this.outx = 0
        this.outy = 0
        this.center = new Circle(200, 500, 100, 'red')
        this.da = 0
        for (let t = 0; t < 12; t++) {

            this.outx = Math.cos(this.da) * (40 + (Math.sin(this.da * sides) * 1))
            this.outy = Math.sin(this.da) * (40 + (Math.sin(this.da * sides) * 1))
            let point = new Circle(this.center.x + this.outx, this.center.y + this.outy, 7, 'yellow')
            point.r = 100
            point.g = 100
            point.b = 0
            this.dots.push(point)
            this.da += ((Math.PI * 2) / 12)
        }

        this.points = []
        this.a = 0
        for (let t = 0; t < 30; t++) {
            this.outx = Math.cos(this.a) * (100 + (Math.sin(this.a * sides) * 30))
            this.outy = Math.sin(this.a) * (100 + (Math.sin(this.a * sides) * 30))
            let point = new Point(this.center.x + this.outx, this.center.y + this.outy)
            this.points.push(point)
            this.a += ((Math.PI * 2) / 30)
        }
    }
    isPointInside(point) {
        return pointInPolygon(point, this.points)
    }
    check(point) {
        for (let t = 0; t < this.dots.length; t++) {
            if (this.dots[t].isPointInside(point)) {
                this.dots[t].dragged *= -1
            }
        }
    }
    draw() {
        if (this.isPointInside(TIP_engine)) {
            this.center.color = "olive"
        } else {
            this.center.color = "tan"

        }
        this.center.draw()
        let inll = new LineOP(this.points[0], this.points[this.points.length - 1], 'white', 3)
        inll.draw()
        for (let t = 0; t < this.dots.length; t++) {
            this.dots[t].color = `rgb(${this.dots[t].r}, ${this.dots[t].g}, ${this.dots[t].b})`

            if (Math.random() < .1) {
                let ran = (Math.random() - .5) * 25
                this.dots[t].g += ran
                this.dots[t].r -= ran
            }
            this.dots[t].r = Math.min(Math.max(this.dots[t].r, 0), 255)
            this.dots[t].g = Math.min(Math.max(this.dots[t].g, 0), 255)
            this.dots[t].draw()
        }
        for (let t = 0; t < this.points.length - 1; t++) {
            let inll = new LineOP(this.points[t], this.points[t + 1], 'white', 3)
            inll.draw()
            // let dot = new Circle(this.points[t].x, this.points[t].y, 3, 'white')
            // dot.draw()
        }
    }
}


// Optimized Audio Timeline with performance improvements
class AudioTimeline {
    constructor() { 
        this.canvas = document.createElement('canvas');
        this.canvas.width = 1280;
        this.canvas.height = 100; // Match rect1 height
        this.canvas.style.position = 'absolute';
        this.canvas.style.top = '90px'; // Match rect1 y
        this.canvas.style.left = '0';
        this.canvas.style.zIndex = '10';
        this.canvas.style.pointerEvents = 'none';
        this.ctx = this.canvas.getContext('2d');
        this.audioContext = new (window.AudioContext || window.webkitAudioContext)();
         
        // Offscreen canvases for persistent waveforms
        this.offscreenCanvases = [];
        this.analysers = [];
        this.dataArrays = [];
        this.sources = [];
        this.streams = [];
        
        this.bufferLength = 1024; // For time domain data
        this.duration = 180; // 3 minutes in seconds
        this.pixelsPerSecond = 10; // Reduced to 10 pixels/second for slower scroll
        this.offscreenWidth = this.duration * this.pixelsPerSecond; // 180 * 10 = 1800 pixels
        this.timeOffset = 0; // Tracks current time position
        this.lastFrameTime = performance.now();
        this.sliceWidth = 1; // 1 pixel per frame at ~60fps (adjusted in rendering)
        
        this.animationFrameId = null;
        this.isRendering = false;

        if (this.audioContext.state === 'suspended') {
            const resume = () => this.audioContext.resume().then(() => console.log("Timeline AudioContext resumed"));
            ["click", "keydown", "touchstart", "pointerdown"].forEach(evt => {
                document.addEventListener(evt, resume, { once: true });
            });
        }
    }

    addStream(stream, label = 'Unknown') {
        if (this.streams.some(s => s === stream)) return; // Avoid duplicates
        
        const analyser = this.audioContext.createAnalyser();
        analyser.fftSize = this.bufferLength * 2; // For time domain
        const dataArray = new Uint8Array(analyser.frequencyBinCount);
        
        const source = this.audioContext.createMediaStreamSource(stream);
        source.connect(analyser);
        
        // Create offscreen canvas for this track
        const offscreen = document.createElement('canvas');
        offscreen.width = this.offscreenWidth; // 1800 pixels for 180 seconds
        offscreen.height = this.canvas.height / 2; // Split for two tracks
        const offCtx = offscreen.getContext('2d');
        offCtx.fillStyle = '#000';
        offCtx.fillRect(0, 0, offscreen.width, offscreen.height);
        
        this.analysers.push(analyser);
        this.dataArrays.push(dataArray);
        this.sources.push(source);
        this.streams.push(stream);
        this.offscreenCanvases.push({canvas: offscreen, ctx: offCtx, label});
        
        if (!this.isRendering) {
            this.renderTimeline();
        }
        console.log(`Added stream: ${label}`);
    }

    renderTimeline() {
        this.animationFrameId = requestAnimationFrame(() => this.renderTimeline());
        this.isRendering = true;

        const currentTime = performance.now();
        const deltaTime = (currentTime - this.lastFrameTime) / 1000; // Seconds since last frame
        this.lastFrameTime = currentTime;
        
        // Update time offset
        this.timeOffset += deltaTime;
        if (this.timeOffset >= this.duration) {
            this.timeOffset -= this.duration; // Loop back to keep within 180 seconds
            // Optional: Reset offscreen canvases to clear old data
            this.offscreenCanvases.forEach(off => {
                off.ctx.fillStyle = '#000';
                off.ctx.fillRect(0, 0, off.canvas.width, off.canvas.height);
            });
        }

        const trackHeight = this.canvas.height / this.offscreenCanvases.length;

        this.offscreenCanvases.forEach((off, index) => {
            const analyser = this.analysers[index];
            const dataArray = this.dataArrays[index];
            analyser.getByteTimeDomainData(dataArray);

            const offCtx = off.ctx;
            
            // Calculate x position for new data (right edge of offscreen canvas)
            const scrollStep = this.pixelsPerSecond * deltaTime; // Pixels to scroll per frame
            const x = ((this.timeOffset * this.pixelsPerSecond) + off.canvas.width - 1) % off.canvas.width; // Right-to-left
            
            // Clear previous column with decay (fade old data)
            offCtx.fillStyle = 'rgba(0, 0, 0, 0.1)'; // Slight fade for decay
            offCtx.fillRect(x, 0, 1, off.canvas.height);
            
            // Draw new waveform slice
            offCtx.strokeStyle = index === 0 ? '#0f0' : '#f00'; // Green for local, red for remote
            offCtx.lineWidth = 1;
            offCtx.beginPath();
            
            for (let i = 0; i < dataArray.length; i += Math.floor(dataArray.length / off.canvas.height)) {
                const v = (dataArray[i] / 128.0) - 1; // -1 to 1
                const y = (trackHeight / 2) + (v * (trackHeight / 2) * 0.5); // Reduce amplitude for visibility
                if (i === 0) {
                    offCtx.moveTo(x, y);
                } else {
                    offCtx.lineTo(x, y);
                }
            }
            
            offCtx.stroke();
            
            // Copy the last 1280 pixels to main canvas, scaled from 1800
            this.ctx.clearRect(0, index * trackHeight, this.canvas.width, trackHeight);
            const scaleFactor = this.canvas.width / this.offscreenWidth; // 1280 / 1800 ‚âà 0.711
            const sourceX = Math.max(0, Math.floor((x - this.canvas.width / scaleFactor) % this.offscreenWidth));
            this.ctx.drawImage(
                off.canvas,
                sourceX,
                0,
                this.canvas.width / scaleFactor,
                off.canvas.height,
                0,
                index * trackHeight,
                this.canvas.width,
                trackHeight
            );
            
            // Draw label
            this.ctx.fillStyle = '#fff';
            this.ctx.font = '12px Arial';
            this.ctx.fillText(off.label, 10, (index * trackHeight) + 20);
        });
    }

    stopRendering() {
        if (this.animationFrameId) {
            cancelAnimationFrame(this.animationFrameId);
            this.animationFrameId = null;
        }
        this.isRendering = false;
    }
}
// Reduced timeline update frequency in main loop
let timelineUpdateCounter = 0;
const TIMELINE_UPDATE_INTERVAL = 2; // Update every 2 frames instead of every frame

// function updateAudioTimeline() {
//     if (!audioTimeline) {
//         if (globalAudioContext && pc && pc.connectionState === 'connected') {
//             initializeAudioTimeline();
//         }
//         return;
//     }
    
//     // Update logic every frame
//     audioTimeline.update();
    
//     // But only draw every few frames
//     // timelineUpdateCounter++;
//     // if (timelineUpdateCounter >= TIMELINE_UPDATE_INTERVAL) {
//         audioTimeline.draw();
//     //     timelineUpdateCounter = 0;
//     // }
// }

// Keep the same initialization and control functions
function initializeAudioTimeline() {
    if (!window.audioTimeline) {
        window.audioTimeline = new AudioTimeline();
        document.body.appendChild(window.audioTimeline.canvas);
        
        // Add local mic if available
        if (globalMicStream) {
            window.audioTimeline.addStream(globalMicStream, 'Local');
        }
        
        // Add existing remote streams
        if (window.pc) {
            const receivers = window.pc.getReceivers();
            receivers.forEach((receiver, index) => {
                if (receiver.track && receiver.track.kind === 'audio') {
                    const stream = new MediaStream([receiver.track]);
                    window.audioTimeline.addStream(stream, `Remote ${index + 1}`);
                    console.log(`Connected existing remote stream ${index + 1} to timeline`);
                }
            });
        } else {
            console.warn("No pc available for remote streams");
        }
    }
}

let audioTimeline = null
let initaud=0 
function handleTimelineControls() {
    if (keysPressed['t']) {
        initializeAudioTimeline();
    }
    if (keysPressed['y']) {
        if (window.audioTimeline) {
            window.audioTimeline.stopRendering();
            window.audioTimeline.canvas.remove();
            window.audioTimeline = null;
        }
    }
}


// Add this to your existing main() function:
// updateAudioTimeline(); // Add after canvas clearing
// handleTimelineControls(); // Add with other key handlers



let room = new RoomShape(5)
let pix = canvas_context.getImageData(0, 0, 1280, 720)
function indexer(point, width) {
    const x = Math.floor(point.x);
    const y = Math.floor(point.y);
    return (y * width + x) * 4;
}
function addingto(nodeon) {
    addingOn = nodeon
    adding = 1
    startRecording()
}
function trimVersion(input) {
    if (typeof input == 'number') return input; // numeric 0 stays 0

    if (typeof input === "string") {
        if (input === "0") return 0; // string "0" becomes numeric 0

        let result = input.slice(0, -9); // remove last two characters
        return result === "0" ? 0 : result;
    }

    throw new Error("Input must be a string or 0");
}

let pausedex = -1
let session = (Math.random() * 100000)

async function sendAudioElement(id, audioElement, colorin, xin = 320, yin = 320, width = -1) {
    if (!audioElement || !audioElement.src) {
        console.error("sendAudioElement: missing audio element or src", audioElement);
        return;
    }

    try {
        // Fetch audio data from the audio element's src (blob/object URL or remote URL)
        const response = await fetch(audioElement.src);
        const audioBlob = await response.blob();
        const audioBuffer = await audioBlob.arrayBuffer();

        // Encode metadata
        console.log(id) 
        const metadata = JSON.stringify({ type: "audio", ID: id, usercolor: colorin + (width > -1 ? '' : ''), resend: 1, x: xin, y: yin, width: width });
        const encoder = new TextEncoder();
        const metadataBytes = encoder.encode(metadata);

        // Combined buffer: [metadata length (4 bytes)] + [metadata] + [audio]
        const totalLength = 4 + metadataBytes.byteLength + audioBuffer.byteLength;
        const combined = new Uint8Array(totalLength);

        // Write metadata length (little-endian)
        const view = new DataView(combined.buffer);
        view.setUint32(0, metadataBytes.byteLength, true);

        // Copy metadata + audio
        combined.set(metadataBytes, 4);
        combined.set(new Uint8Array(audioBuffer), 4 + metadataBytes.byteLength);

        // Send
        robust.send(combined.buffer);

        //   console.log("Audio element sent successfully, id:", id);
    } catch (error) {
        console.error("Failed to send audio element:", error);
    }
}


// Ensure global storage exists
// mouseOverPrinterObject = mouseOverPrinterObject || [];
















let mouseOverPrinterObject = {};

function initAudioContext() {
  if (!globalAudioContext) {
    try {
      globalAudioContext = new (window.AudioContext || window.webkitAudioContext)();
      console.log("AudioContext created, state:", globalAudioContext.state);
    } catch (e) {
      console.warn("Failed to create AudioContext:", e);
      globalAudioContext = null;
    }
  }
  return globalAudioContext;
}
function resumeAudioContext() {
  const ctx = Tone.getContext().rawContext;
  if (ctx.state === "suspended") {
    ctx.resume().then(() => {
      console.log("AudioContext resumed after user gesture");
    }).catch(err => {
      console.error("Failed to resume AudioContext:", err);
    });
  }
}

["click", "touchstart", "keydown", "pointerdown"].forEach(evt => {
  document.addEventListener(evt, resumeAudioContext, { once: true });
});




function exposeToGlobal() {
    try {
        window.pc = pc || null;
        window.signaling = signaling || {};
        window.webrtcInitialized = webrtcInitialized || false;
        window.isPolite = isPolite || false;
        window.makingOffer = makingOffer || false;
        window.ignoreOffer = ignoreOffer || false;
        window.pendingIceCandidates = pendingIceCandidates || [];
        window.robust = typeof robust !== 'undefined' ? robust : null;
        window.globalAudioContext = globalAudioContext || null;
        window.audioContextReady = audioContextReady || false;
        
        // These are the missing functions causing the error:
        window.scheduleMessageProcessing = scheduleMessageProcessing;
        window.processSignalingMessage = processSignalingMessage;
        window.messageQueue = messageQueue || [];
        window.processingMessages = processingMessages || false;
        window.pendingSignalingMessages = pendingSignalingMessages || [];
        window.processMessage = processMessage;
        window.processMessageQueue = processMessageQueue;
        
    } catch (e) {
        console.warn("Failed to expose globals:", e);
    }
}

// Call immediately after definition
exposeToGlobal();

// Create a single empty buffer to reuse
let emptyBuffer = null;

function getEmptyBuffer() {
  const ctx = initAudioContext();
  if (!ctx) {
    // Fallback: return a minimal mock buffer if AudioContext fails
    return { duration: 0, sampleRate: 44100, numberOfChannels: 1, length: 1 };
  }
  
  if (!emptyBuffer) {
    try {
      emptyBuffer = ctx.createBuffer(1, 1, ctx.sampleRate);
    } catch (e) {
      console.warn("Failed to create empty buffer:", e);
      return { duration: 0, sampleRate: 44100, numberOfChannels: 1, length: 1 };
    }
  }
  return emptyBuffer;
}

let audioLock = 1 //turns off chimes
async function makenoise(hexcode) {
  // If locked, return cached empty buffer immediately
  if (audioLock === 1) {
    return getEmptyBuffer();
  }

  // Strip '#' if present
  hexcode = hexcode.replace(/^#/, '').toUpperCase();

  // Take first 6 chars from hexcode, repeat if shorter
  const chars = Array.from({ length: 6 }, (_, i) => hexcode[i % hexcode.length]);

  const toneAudioBuffer = await Tone.Offline((context) => {
    const synth = new Tone.PolySynth(Tone.Synth, {
      oscillator: { type: "triangle" },
      volume: -5
    }).toDestination();

    // First three notes: play normally on time
    chars.slice(0, 3).forEach((ch, i) => {
      const note = hexNotes[ch] || "C4";
      synth.triggerAttackRelease(note, 0.1, i * 0.20);
    });

    // Last three notes: play simultaneously with first three but offset by 0.125s
    chars.slice(3, 6).forEach((ch, i) => {
      const note = hexNotes[ch] || "C4";
      synth.triggerAttackRelease(note, 0.1, i * 0.20 + 0.1);
    });

  }, 4); // offline duration in seconds

  const nativeBuffer = toneAudioBuffer.get ? toneAudioBuffer.get() : toneAudioBuffer._buffer;

  // Optional: visualize
  // visualizeAudioBuffer(nativeBuffer, canvas, 50, 50, 400, 100);

  return nativeBuffer;
}



































async function visualizeAudioNode(audio) {
    if (!(audio instanceof HTMLAudioElement || audio instanceof Audio)) {
        throw new Error("Expected an Audio object");
    }

    // Create offscreen canvas
    const canvas = document.createElement('canvas');
    canvas.width = 300;
    canvas.height = 100;
    const ctx = canvas.getContext('2d');

    // Reuse or create AudioContext
    if (!globalAudioContext) {
        globalAudioContext = new (window.AudioContext || window.webkitAudioContext)();
    }
    
    // Resume context if suspended (required for user interaction)
    if (globalAudioContext.state === 'suspended') {
        await globalAudioContext.resume();
    }

    // Load the audio file as a buffer instead of hijacking the audio element
    let audioBuffer;
    try {
        const response = await fetch(audio.src);
        const arrayBuffer = await response.arrayBuffer();
        audioBuffer = await globalAudioContext.decodeAudioData(arrayBuffer);
    } catch (error) {
        console.error('Could not load audio for visualization:', error);
        throw error;
    }

    // Create buffer source and analyser
    const source = globalAudioContext.createBufferSource();
    const analyser = globalAudioContext.createAnalyser();
    const gainNode = globalAudioContext.createGain();
    
    // Set up the buffer source
    source.buffer = audioBuffer;
    source.loop = false;
    
    // Better settings for visualization
    analyser.fftSize = 256;
    analyser.smoothingTimeConstant = 0.95;
    
    // Set gain to 0 for silent processing
    gainNode.gain.setValueAtTime(0, globalAudioContext.currentTime);
    
    // Connect: buffer source -> analyser -> gain(0) -> destination
    source.connect(analyser);
    analyser.connect(gainNode);
    gainNode.connect(globalAudioContext.destination);

    const bufferLength = analyser.frequencyBinCount;
    const dataArray = new Uint8Array(bufferLength);

    // Start the buffer source
    source.start(0);
    
    let animationId;
    let startTime = Date.now();

    // Draw function
    function draw() {
        animationId = requestAnimationFrame(draw);
        
        // Use frequency data instead of time domain for more visible results
        analyser.getByteFrequencyData(dataArray);

        // Partial clear with fade effect
        ctx.fillStyle = 'rgba(0, 0, 0, 0.04)';
        ctx.fillRect(0, 0, canvas.width, canvas.height);
        
        // Calculate color based on elapsed time
        const elapsedTime = (Date.now() - startTime) / 1000; // seconds
        const colorPhase = Math.min(elapsedTime / audioBuffer.duration, 1); // Clamp to 0-1
        let strokeColor;
        
        if (colorPhase < 0.33) {
            // Green to Yellow transition (first 1/3 of duration)
            const t = colorPhase / 0.33;
            const r = Math.floor(255 * t);
            const g = 255;
            const b = 0;
            strokeColor = `rgb(${r}, ${g}, ${b})`;
        } else if (colorPhase < 0.66) {
            // Yellow to Red transition (middle 1/3 of duration)
            const t = (colorPhase - 0.33) / 0.33;
            const r = 255;
            const g = Math.floor(255 * (1 - t));
            const b = 0;
            strokeColor = `rgb(${r}, ${g}, ${b})`;
        } else {
            // Red fade (final 1/3 of duration)
            const t = (colorPhase - 0.66) / 0.34;
            const r = 255;
            const g = 0;
            const b = 0;
            strokeColor = `rgb(${r}, ${g}, ${b})`;
        }
        
        ctx.lineWidth = 2;
        ctx.strokeStyle = strokeColor;
        ctx.beginPath();

        const sliceWidth = canvas.width / bufferLength;
        let x = 0;

        for (let i = 0; i < bufferLength; i++) {
            const v = dataArray[i] / 255.0; // normalize to 0..1
            const y = canvas.height - (v * canvas.height); // Invert y so peaks go up

            if (i === 0) {
                ctx.moveTo(x, y);
            } else {
                ctx.lineTo(x, y);
            }
            x += sliceWidth;
        }

        ctx.stroke();

        // Stop animation when audio buffer is done
        if (elapsedTime >= audioBuffer.duration) {
            cancelAnimationFrame(animationId);
        }
    }

    draw();

    // Store in global storage with cleanup function
    mouseOverPrinterObject = { 
        context: ctx, 
        canvas,
        audio, // Original audio element unchanged
        cleanup: () => {
            if (animationId) {
                cancelAnimationFrame(animationId);
            }
            // Source will stop automatically when buffer ends
        }
    };

    return mouseOverPrinterObject;
}


// Example usage:
// const audio = new Audio('your-audio-file.mp3');
// audio.src = 'path/to/your/audio.mp3';
// visualizeAudioNode(audio);




function funyk() {

    getAudioFile().then(file => {
        console.log("User selected file:", file);
        ///file

        fileon = file
        rect1.color = 'red'
        visualizeAudio(fileon, zffcanvas, rect1.x, rect1.y, rect1.width, rect1.height)

    }).catch(err => {
        console.error(err);
    });
}

let mouseoverwindow = new Rectangle(0,0, 100, 40, 'transparent')
let mouseovertarget = {}
mouseovertarget.body = {}
mouseovertarget.body.x = 0
mouseovertarget.body.y = 0

async function visualizeAudio(file, canvas, x = 0, y = 0, width = canvas.width, height = canvas.height) {
    const ctx = canvas.getContext("2d");
    const audioContext = new (window.AudioContext || window.webkitAudioContext)();

    // Read file as ArrayBuffer
    const arrayBuffer = await file.arrayBuffer();

    // Decode audio data
    const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
    const rawData = audioBuffer.getChannelData(0); // Use first channel

    const step = Math.ceil(rawData.length / width); // Number of samples per pixel

    // Clear the specified rectangle
    ctx.clearRect(x, y, width, height);

    // Draw background for the rectangle
    ctx.fillStyle = "#222";
    ctx.fillRect(x, y, width, height);

    // Draw waveform
    ctx.strokeStyle = "#0f0";
    ctx.lineWidth = 1;
    ctx.beginPath();

    for (let i = 0; i < width; i++) {
        const start = i * step;
        const end = Math.min(start + step, rawData.length);
        let min = 1.0;
        let max = -1.0;

        for (let j = start; j < end; j++) {
            const sample = rawData[j];
            if (sample < min) min = sample;
            if (sample > max) max = sample;
        }

        const y1 = y + ((1 + min) / 2) * height;
        const y2 = y + ((1 + max) / 2) * height;

        ctx.moveTo(x + i, y1);
        ctx.lineTo(x + i, y2);
    }

    ctx.stroke();
}

// Usage example:
// const fileInput = document.querySelector("#audioFileInput");
// const canvas = document.querySelector("#waveformCanvas");
// fileInput.addEventListener("change", e => visualizeAudio(e.target.files[0], canvas, 50, 50, 400, 100));
async function clipAudio(fileOrBlob, startTime, endTime) {
    const audioCtx = new (window.AudioContext || window.webkitAudioContext)();
    let arrayBuffer;

    if (fileOrBlob instanceof Blob) {
        arrayBuffer = await fileOrBlob.arrayBuffer();
    } else {
        throw new Error("clipAudio expects a File or Blob");
    }

    const decoded = await audioCtx.decodeAudioData(arrayBuffer);

    startTime = Math.max(0, startTime);
    endTime = Math.min(decoded.duration, endTime);
    if (endTime <= startTime) throw new Error("Invalid clip range");

    const duration = endTime - startTime;
    const sampleRate = decoded.sampleRate;
    const channels = decoded.numberOfChannels;

    const clipped = audioCtx.createBuffer(
        channels,
        Math.floor(duration * sampleRate),
        sampleRate
    );

    for (let ch = 0; ch < channels; ch++) {
        const data = decoded.getChannelData(ch).subarray(
            Math.floor(startTime * sampleRate),
            Math.floor(endTime * sampleRate)
        );
        clipped.copyToChannel(data, ch);
    }

    // convert AudioBuffer -> WAV Blob
    const wavBlob = audioBufferToWav(clipped); // same helper as before
    return wavBlob; // <-- return a Blob
}

// helper: convert AudioBuffer ‚Üí WAV Blob
function audioBufferToWav(buffer) {
    const numOfChan = buffer.numberOfChannels,
        length = buffer.length * numOfChan * 2 + 44,
        bufferArray = new ArrayBuffer(length),
        view = new DataView(bufferArray),
        channels = [],
        sampleRate = buffer.sampleRate;

    // write WAV header
    function setUint16(data, offset) {
        view.setUint16(offset, data, true);
    }
    function setUint32(data, offset) {
        view.setUint32(offset, data, true);
    }
    let pos = 0;

    setUint32(0x46464952, pos); // "RIFF"
    pos += 4;
    setUint32(length - 8, pos);
    pos += 4;
    setUint32(0x45564157, pos); // "WAVE"
    pos += 4;
    setUint32(0x20746d66, pos); // "fmt "
    pos += 4;
    setUint32(16, pos);
    pos += 4;
    setUint16(1, pos);
    pos += 2;
    setUint16(numOfChan, pos);
    pos += 2;
    setUint32(sampleRate, pos);
    pos += 4;
    setUint32(sampleRate * 2 * numOfChan, pos);
    pos += 4;
    setUint16(numOfChan * 2, pos);
    pos += 2;
    setUint16(16, pos);
    pos += 2;
    setUint32(0x61746164, pos); // "data"
    pos += 4;
    setUint32(length - pos - 4, pos);
    pos += 4;

    // write interleaved samples
    for (let i = 0; i < buffer.numberOfChannels; i++)
        channels.push(buffer.getChannelData(i));

    let offset = 0;
    while (offset < buffer.length) {
        for (let i = 0; i < numOfChan; i++) {
            let sample = Math.max(-1, Math.min(1, channels[i][offset]));
            sample = sample < 0 ? sample * 0x8000 : sample * 0x7fff;
            view.setInt16(pos, sample, true);
            pos += 2;
        }
        offset++;
    }

    return new Blob([bufferArray], { type: "audio/wav" });
}

let hovercheck = 1

hoverTarget = nodes[0]


let timestep = 100
let stopper = 0
let timeip = 60

function childSearch(children, color){

    let c = 0
    for(let t= 0;t<children.length;t++){
        if(children[t].usercolor == color){
            c++
        }
    }

    return c

}


async function getMicStream() {
  try {
    // Use the fresh stream from resetWebRTCState, or get a new one
    if (globalMicStream && globalMicStream.active) {
      return globalMicStream;
    }
    
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
    globalMicStream = stream;
    return stream;
  } catch (err) {
    console.error("Microphone access denied:", err);
    return null;
  }
}

function playRemoteStream(stream) {
  const audio = document.createElement("audio");
  audio.srcObject = stream;
  audio.autoplay = true;
  document.body.appendChild(audio); // hidden playback
}


// Play incoming audio


async function initWebRTC() {
    const pcInstance = new RTCPeerConnection({
        iceServers: [
            // Google STUNs
            { urls: "stun:stun.l.google.com:19302" },
            { urls: "stun:stun1.l.google.com:19302" },
            { urls: "stun:stun2.l.google.com:19302" },
            { urls: "stun:stun3.l.google.com:19302" },
            { urls: "stun:stun4.l.google.com:19302" },

            // Free TURN (metered.ca relay)
            {
                urls: [
                    "turn:openrelay.metered.ca:80",
                    "turn:openrelay.metered.ca:443",
                    "turn:openrelay.metered.ca:443?transport=tcp"
                ],
                username: "openrelayproject",
                credential: "openrelayproject"
            },

            // Another community/free STUN
            { urls: "stun:stun.stunprotocol.org:3478" },
            { urls: "stun:stun.services.mozilla.com" }
        ]
    });

    // Debug helper: test audio tracks & states
    window.testWebRTCAudio = function () {
        if (!pcInstance) {
            console.warn("No pcInstance available for testWebRTCAudio");
            return;
        }
        console.log("WebRTC Connection States:", {
            connection: pcInstance.connectionState,
            ice: pcInstance.iceConnectionState,
            signaling: pcInstance.signalingState,
        });

        console.log("Local senders:", pcInstance.getSenders().length);
        console.log("Remote receivers:", pcInstance.getReceivers().length);

        pcInstance.getReceivers().forEach((receiver, i) => {
            if (receiver.track && receiver.track.kind === "audio") {
                console.log(`Remote audio track ${i}:`, {
                    enabled: receiver.track.enabled,
                    muted: receiver.track.muted,
                    readyState: receiver.track.readyState,
                    id: receiver.track.id,
                });
            }
        });

        pcInstance.getStats().then(stats => {
            stats.forEach(report => {
                if (report.type === "candidate-pair" && report.state === "succeeded") {
                    console.log("Selected candidate pair:", {
                        local: report.localCandidateId,
                        remote: report.remoteCandidateId,
                        nominated: report.nominated,
                        bytesSent: report.bytesSent,
                        bytesReceived: report.bytesReceived,
                    });
                }
            });
        });
    };

    // Enhanced ontrack handler that integrates with audio timeline
// In initWebRTC() - update ontrack to add to timeline
pcInstance.ontrack = (event) => {
    if (event.track.kind !== "audio") return;

    console.log("Remote audio track received:", {
        enabled: event.track.enabled,
        muted: event.track.muted,
        readyState: event.track.readyState,
        id: event.track.id,
    });

    // Setup audio element for playback
    let audioEl = document.getElementById("remoteAudio");
    if (!audioEl) {
        audioEl = document.createElement("audio");
        audioEl.id = "remoteAudio";
        audioEl.autoplay = true;
        audioEl.controls = true;
        audioEl.muted = false;
        audioEl.volume = 1.0;
        document.body.appendChild(audioEl);
    }

    audioEl.srcObject = event.streams[0];
    audioEl.muted = false;
    audioEl.volume = 1.0;

    const tryPlay = () => audioEl.play().catch(() => {});
    tryPlay();

    // Connect to audio timeline if it exists
    if (window.audioTimeline) {
        window.audioTimeline.addStream(event.streams[0], 'Remote');
        console.log(6); // This should now hit when new tracks arrive
    }

    event.track.onunmute = () => {
        tryPlay();
    };

    audioEl.onloadedmetadata = () => {
        tryPlay();
    };

    const poll = setInterval(() => {
        if (!event.track.muted) {
            tryPlay();
            clearInterval(poll);
        }
    }, 500);
};
    pcInstance.onicecandidate = (event) => {
        if (event.candidate) {
            robust.send(JSON.stringify({ candidate: event.candidate }));
            // console.log("ICE candidate:", event.candidate.candidate);
        }
    };

    pcInstance.onconnectionstatechange = () => {
        // console.log("Connection state:", pcInstance.connectionState);
        // console.log("ICE connection state:", pcInstance.iceConnectionState);
        // console.log("ICE gathering state:", pcInstance.iceGatheringState);
        
        // Auto-initialize timeline when WebRTC connects
        if (pcInstance.connectionState === 'connected') {
            if (!window.audioTimeline) {
                setTimeout(() => {
                    initializeAudioTimeline();
                    // console.log("Audio timeline auto-initialized on WebRTC connection");
                }, 1000);
            } else {
                // If timeline already exists, try to connect existing streams
                const receivers = pcInstance.getReceivers();
                receivers.forEach(receiver => {
                    if (receiver.track && receiver.track.kind === 'audio') {
                        const stream = new MediaStream([receiver.track]);
                        window.audioTimeline.connectToRemoteStream(stream);
                        console.log("Connected existing WebRTC stream to timeline");
                    }
                });
            }
        }
    };

    return pcInstance;
}



// Add your microphone stream
let stream


// // When a remote track comes in, play it
// pc.ontrack = (event) => { 
//   const audio = document.createElement("audio");
//   audio.srcObject = event.streams[0];
//   audio.autoplay = true;
//   document.body.appendChild(audio);
// };


// pc.onicecandidate = (event) => {
//   if (event.candidate) {
//     sendSignal("candidate", event.candidate);
//   }
// };


async function startCall() {
  const offer = await pc.createOffer();
  await pc.setLocalDescription(offer);
  ws.send(JSON.stringify({ offer }));
}


// Now you need signaling (via WebSocket or any server) to exchange
// pc.createOffer(), pc.createAnswer(), and ICE candidates





function sendSignal(type, payload) {
  robust.send(JSON.stringify({ [type]: payload }));
}


function testWebRTCAudio() {
  if (pc) {
    console.log("WebRTC Connection States:", {
      connection: pc.connectionState,
      ice: pc.iceConnectionState,
      signaling: pc.signalingState
    });
    
    console.log("Local senders:", pc.getSenders().length);
    console.log("Remote receivers:", pc.getReceivers().length);
    
    // Check if we have remote streams
    pc.getReceivers().forEach((receiver, i) => {
      if (receiver.track && receiver.track.kind === 'audio') {
        console.log(`Remote audio track ${i}:`, {
          enabled: receiver.track.enabled,
          muted: false, //receiver.track.muted,
          readyState: receiver.track.readyState
        });
      }
    });
  }
}

// Add this to check what's happening on both sides
function debugWebRTCState() {
  console.log("=== WebRTC Debug Info ===");
  console.log("Signaling state:", pc?.signalingState);
  console.log("Connection state:", pc?.connectionState);
  console.log("ICE state:", pc?.iceConnectionState);
  console.log("Pending ICE candidates:", pendingIceCandidates?.length || 0);
  console.log("Has remote description:", !!pc?.remoteDescription);
  console.log("Has local description:", !!pc?.localDescription);
}
const remoteAudio = document.createElement("audio");
remoteAudio.autoplay = true;
remoteAudio.controls = true;
remoteAudio.volume = 1.0;
remoteAudio.muted = false;
document.body.appendChild(remoteAudio);

// Ensure AudioContext resumes after gesture
function ensureAudioContext() {
  const ctx = Tone.getContext().rawContext;
  if (ctx.state === "suspended") {
    document.addEventListener("click", () => ctx.resume(), { once: true });
    document.addEventListener("keydown", () => ctx.resume(), { once: true });
  }
}
ensureAudioContext();
function enableMic() {
  globalMicStream?.getAudioTracks().forEach(t => {
    t.enabled = true;
  });
  console.log("üéôÔ∏è Mic enabled");
}

function disableMic() {
  globalMicStream?.getAudioTracks().forEach(t => {
    t.enabled = false;
  });
  console.log("ü§´ Mic disabled");
}

// Example: hold Z for PTT
window.addEventListener("keydown", e => {
  if (e.code === "KeyZ") {
    enableMic();
  }
});
window.addEventListener("keyup", e => {
  if (e.code === "KeyZ") {
    disableMic();
  }
});



async function main() {

  handleTimelineControls()
  // console.log(nodes)
    timeip++
    if(timeip > timestep){
        let lister = []
        timeip = 0
        for(let t = 0;t<nodes.length;t++){
            lister.push(nodes[t].ID)
        }
        robust.send(JSON.stringify({resendList:1,please:lister}))

    }
    stopper++
        if(stopper > (Math.ceil(Math.abs(nodes.length)))*timestep*10){
            stopper =0
            for(let k = 0;k<nodes.length;k++){
                nodes[k].sending  = 1
            }
            // testMicrophone()
        }

  hovercheck--
//   if(hovercheck <= 0){
//     hovercheck = 30
//     let l = new LineOP(TIP_engine, TIP_engine);
//     let min = 99999999;
//     let index = -1;
    
//     for (let t = 0; t < nodes.length; t++) {
//         l.target = nodes[t].cap;
//         let h = l.hypotenuse();
//         if (h <= min && h <= nodes[t].offset.radius) {
//             index = t;
//             min = h;
//         }
//     }

//     // Only update hoverTarget if we found a valid node, otherwise keep current or set to null
//     if (index > -1) {
//         hoverTarget = nodes[index];
//     } else {
//         hoverTarget = null; // Clear hover target when no node is in range
//     }
//   }

  startmouse--
  if(keysPressed['v']){
    coloron = getRandomColor()
  }
  if(lock ==1){
    return
  }
 
  links = []
  timerz++

  if(keysPressed['w']){
      startmouse = 50
      overlap = 50
      topnodes[0].body.y-=10
  }
  if(keysPressed['s']){
      startmouse = 50
      overlap = 50
      topnodes[0].body.y+=10
  }
  if(keysPressed['a']){
      startmouse = 50
      overlap = 50
      topnodes[0].body.x-=10
  }
  if(keysPressed['d']){
      startmouse = 50
      overlap = 50
      topnodes[0].body.x+=10
  }

  if (uploaded == 1) {
      uploaded = 0
      funyk()
  }

  if (timerz % 40 == 0) {
      if (nodes[Math.floor(timerz / 40) % (nodes.length)].ID != 0) {
        if(nodes[Math.floor(timerz / 40) % (nodes.length)].sending != 0){
            sendAudioElement(nodes[Math.floor(timerz / 40) % (nodes.length)].ID, nodes[Math.floor(timerz / 40) % (nodes.length)].content.message, nodes[Math.floor(timerz / 40) % (nodes.length)].usercolor, nodes[Math.floor(timerz / 40) % (nodes.length)].cap.x, nodes[Math.floor(timerz / 40) % (nodes.length)].cap.y, nodes[Math.floor(timerz / 40) % (nodes.length)].width)
        }
      }
  }
  
  off_context.clearRect(0, 0, 1280, 1280)
  off_context.drawImage(canvas, offset.x, 0, 1280, 1280, 0, 0, 1280, 1280)
  canvas_context.clearRect(-1, -1, canvas.width * 1.1, canvas.height * 1.1)
  
  timeon = 0
  if (timespeed <= 0) {
      timespeed = 20
      canvas_context.translate(-1, 0)
      offset.x -= 1
      timeon = 1
      for (let t = 0; t < nodes.length; t++) {
          if (nodes[t].childing == 1) {
              if (nodes[t].childos == 1) {
                  nodes[t].offset.x -= (nodes[t].cap.x - TIP_engine.x) / 2
                  nodes[t].offset.y -= (nodes[t].cap.y - TIP_engine.y) / 2
              }
          }
      }
  }

  if (!(movedMouse == 1 || startmouse > 0)) {
      canvas_context.clearRect(-1, -1, canvas.width * 1.1, canvas.height * 1.1)
      canvas_context.drawImage(offcanvas, 0, 0, 1280, 1280, 0, 0, 1280, 1280)

      let wet = 0
      for(let t = 0; t < nodes.length; t++){
        if (nodes[t] == addingOn) {
          wet = 1
        }
      }

      // // Only draw hover effect if we have a valid hoverTarget and not adding
      // if(wet == 0 && hoverTarget !== null && hoverTarget !== undefined){
      //   if(hoverTarget.malice != 1){
      //     hoverTarget.malice = 1
      //     visualizeAudioNode(hoverTarget.content.message)
      //   }
        
        
      //   // Calculate the actual drawing position with offset
      //   let drawX = hoverTarget.body.x + hoverTarget.offset.x;
      //   let drawY = hoverTarget.body.y + hoverTarget.offset.y;
        
      //   // Scale by radius - diameter becomes height, width scales proportionally
      //   let diameter = hoverTarget.offset.radius * 2;
      //   let scaleFactor = diameter / mouseOverPrinterObject.canvas.height;
      //   let scaledWidth = mouseOverPrinterObject.canvas.width * scaleFactor;
      //   let scaledHeight = diameter;
        
      //   // Draw top half (normal)
      //   canvas_context.drawImage(
      //       mouseOverPrinterObject.canvas, 
      //       0, 0, mouseOverPrinterObject.canvas.width, mouseOverPrinterObject.canvas.height, 
      //       drawX, drawY - (scaledHeight/2), 
      //       scaledWidth, scaledHeight/2
      //   );

      //   // Draw bottom half (flipped upside down)
      //   canvas_context.save();
      //   canvas_context.scale(1, -1);
      //   canvas_context.drawImage(
      //       mouseOverPrinterObject.canvas, 
      //       0, 0, mouseOverPrinterObject.canvas.width, mouseOverPrinterObject.canvas.height, 
      //       drawX, -(drawY + (scaledHeight/2)), 
      //       scaledWidth, scaledHeight/2
      //   );
      //   canvas_context.restore();
      // }
      
  } else if (movedMouse == 1 || startmouse > 0) {
      made--
      movedMouse = 0
      canvas_context.clearRect(-1, -1, canvas.width * 1.1, canvas.height * 1.1)
      rect1.draw()
      
      if (fileon != false) {
          canvas_context.drawImage(zffcanvas, 0, 0, 1280, 1280, 0, 0, 1280, 1280)
      }
      
      for (let t = 0; t < highs.length; t++) {
          highs[t].draw()
      }
      for (let t = 0; t < nodes.length; t++) {
        for(let k = 0;k<nodes[t].children.length;k++){

          if(nodes[t].children[k].collapsed == -1){
             let n =  nodes[t].collapsed = -1
             while(n.parent){
              n.collapsed = -1
             }
            }
        }
          }


      for (let t = 0; t < topnodes.length; t++) {
          drawNodeD(topnodes[t])
      }
      
      for (let t = 0; t < topnodes.length; t++) {
          drawNode(topnodes[t])
      }
      
      for (let t = 0; t < nodes.length; t++) {
          nodes[t].offsetting()
          if (seenIDs.has(nodes[t].ID)) {
              // Already seen
          } else {
              // seenIDs.add(nodes[t].ID)
          }
      }

      // let wet = 0
      // for(let t = 0; t < nodes.length; t++){
      //   if (nodes[t] == addingOn) {
      //     wet = 1
      //   }
      // }

      overlap--
      
      // Only draw hover effect if we have a valid hoverTarget and overlap is active
      // if(wet == 0 && overlap > -1 && hoverTarget !== null && hoverTarget !== undefined){
      //   if(hoverTarget.malice != 1){
      //     hoverTarget.malice = 1
      //     visualizeAudioNode(hoverTarget.content.message)
      //   }
        
      //   // Draw top half (normal)
      //   canvas_context.drawImage(
      //       mouseOverPrinterObject.canvas, 
      //       0, 0, mouseOverPrinterObject.canvas.width, mouseOverPrinterObject.canvas.height, 
      //       hoverTarget.cap.x, hoverTarget.cap.y - (mouseOverPrinterObject.canvas.height/2), 
      //       mouseOverPrinterObject.canvas.width, mouseOverPrinterObject.canvas.height/2
      //   );

      //   // Draw bottom half (flipped upside down)
      //   canvas_context.save();
      //   canvas_context.scale(1, -1);
      //   canvas_context.drawImage(
      //       mouseOverPrinterObject.canvas, 
      //       0, 0, mouseOverPrinterObject.canvas.width, mouseOverPrinterObject.canvas.height, 
      //       hoverTarget.cap.x, -(hoverTarget.cap.y + (mouseOverPrinterObject.canvas.height/2)), 
      //       mouseOverPrinterObject.canvas.width, mouseOverPrinterObject.canvas.height/2
      //   );
      //   canvas_context.restore();
      // }
  }

  // Reset malice flag when not hovering
  // if(hoverTarget === null || hoverTarget === undefined) {
  //   for(let t = 0; t < nodes.length; t++) {
  //     if(nodes[t].malice === 1) {
  //       nodes[t].malice = 0; // Reset malice when no longer hovering
  //     }
  //   }
  // }


  // updateAudioTimeline();


  
}


let recorder;

async function startRecording() {
    recorder = await recordAudio();
    console.log("Recording started...");
}
async function stopRecording() {
    const result = await recorder.stop();
    console.log("Recording stopped!");
    return result; // ‚úÖ return the full object { audioBlob, audioUrl, audio }
}


// helper that creates the recording object 
async function recordAudio() {
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
    const mediaRecorder = new MediaRecorder(stream);
    const audioChunks = [];

    return new Promise((resolve) => {
        mediaRecorder.ondataavailable = (event) => {
            if (event.data.size > 0) {
                audioChunks.push(event.data);
            }
        };

        mediaRecorder.start();

        resolve({
            stop: () =>
                new Promise((res) => {
                    mediaRecorder.onstop = () => {
                        const audioBlob = new Blob(audioChunks, { type: "audio/webm" });
                        const audioUrl = URL.createObjectURL(audioBlob);
                        const audio = new Audio(audioUrl);
                        res({ audioBlob, audioUrl, audio });
                    };
                    mediaRecorder.stop();
                    stream.getTracks().forEach(track => track.stop());
                })
        });
    });
}



// connect to the same WS as your server
// Dynamic WebSocket URL that works locally and on Heroku
// when connected
ws.onopen = () => {
    console.log("WS connected!");
};



// helper: send object + audio file
// helper: send object + audio file


async function sendAudioObject(id, file) {
    if (!file || !file.audioBlob) {
        console.error("sendAudioObject: missing audioBlob", file);
        return;
    }

    try {
        const audioArrayBuffer = await file.audioBlob.arrayBuffer();

        const metadata = JSON.stringify({ type: "audio", ID: id, usercolor: coloron });
        const metadataBytes = new TextEncoder().encode(metadata);

        const totalLength = 4 + metadataBytes.byteLength + audioArrayBuffer.byteLength;
        const combined = new Uint8Array(totalLength);
        const view = new DataView(combined.buffer);

        view.setUint32(0, metadataBytes.byteLength, true);
        combined.set(metadataBytes, 4);
        combined.set(new Uint8Array(audioArrayBuffer), 4 + metadataBytes.byteLength);

        // Send slice of buffer to be safe
        robust.send(combined.buffer.slice(0, totalLength));

        console.log("Audio sent successfully, id:", id);
    } catch (err) {
        console.error("Failed to send audio:", err);
    }
}

// socketize(ws)






// --- replace const ws = new WebSocket(wsUrl); ---
// instead of calling socketize(ws) directly, hook into the wrapper:







async function processMessageQueue() {
  let processed = 0;
  const startTime = performance.now();
  const maxProcessingTime = 8; // max 8ms per batch to avoid blocking
  
  try {
    while (messageQueue.length > 0 && processed < MAX_MESSAGES_PER_BATCH) {
      // Check if we're taking too long
      if (performance.now() - startTime > maxProcessingTime) {
        break;
      }
      
      const workItem = messageQueue.shift();
      await processMessage(workItem);
      processed++;
    }
  } catch (e) {
    console.error("Error processing message batch:", e);
  }
  
  // Schedule next batch if more messages remain
  if (messageQueue.length > 0) {
    setTimeout(() => processMessageQueue(), PROCESSING_INTERVAL);
  } else {
    processingMessages = false;
  }
}

// The actual message processing logic (moved from onRobustMessage)






async function processMessage(workItem) {
  const { event, rawSocket, data } = workItem;
  
  // DEBUG: Log what type of message we're processing
  // console.log("Processing message type:", typeof data, "size:", data.byteLength || data.length);
  if (typeof data === "string") {
    try {
      const parsed = JSON.parse(data);
      // console.log("String message content:", Object.keys(parsed));
    } catch (e) {
      // console.log("Non-JSON string message");
    }
  }
  
  // Skip WebRTC signaling messages from audio processing
  if (typeof data === "string") {
    let parsed;
    try {
      parsed = JSON.parse(data);
      // If it's WebRTC signaling, don't process as audio - already handled by onRobustMessage
      if (parsed.offer || parsed.answer || parsed.candidate) {
        console.log("SKIPPING WebRTC message in processMessage");
        return;
      }
    } catch (e) {
      // Not JSON, continue with normal processing
    }
  }
  
  rawSocket.binaryType = "arraybuffer";
  
  try {
    // ---------- TEXT PACKETS ----------
    if (typeof data === "string") {
      let d;
      try { 
        d = JSON.parse(data); 
      } catch (err) { 
        console.warn("received string but failed to parse JSON", err); 
        return; 
      }

      // Handle resendList first (doesn't need parent node)
      if(d.resendList == 1){
        for(let t = 0;t<d.please.length;t++){
            for(let k = 0;k<nodes.length;k++){
                if(d.please[t] == nodes[k].ID){
                    nodes[k].sending  = 0
                }
            }
        }
        return;
      }

      // find parent node index (node that matches d.ID)
      let indexer = nodes.findIndex(n => n && n.ID == d.ID);

      if (d.resend === 1) {
        if (seenIDs.has(d.ID)) return; // already handled

        // If parent node missing, create root node
        if (indexer === -1) {
          if (typeof d.ID === 'number') {
            const node = new Node(0, {
              message: {},
              x: d.x,
              y: d.y
            });
            const high = new Rectangle(d.x, d.y - rect1.height, d.width, rect1.height, d.usercolor + '60');
            highs.push(high);
            node.ID = d.ID;
            node.width = d.width;
            seenIDs.add(node.ID);
            node.usercolor = d.usercolor;
            node.content.message = d.audio || null;
            if (node.content.message) allaud.push(node.content.message);
            nodes.push(node);
            topnodes.push(node);
            startmouse = 50;
            overlap = 50;
          }
          return;
        }

        // create child node under parent
        const childNode = new Node(0, {
          message: {},
          x: nodes[indexer].cap.x + (Math.random() - 0.5),
          y: nodes[indexer].cap.y + 4
        });
        childNode.ID = (d.ID) + (d.usercolor || "") + "." + (childSearch(nodes[indexer].children , d.usercolor)+ 1);
        seenIDs.add(childNode.ID);
        childNode.usercolor = d.usercolor;
        childNode.content.message = d.audio || null;
        childNode.messageType = "text";
        nodes[indexer].children.push(childNode);
        nodes.push(childNode);
        startmouse = 50;
        overlap = 50;

      // NORMAL (non-resend) branch
      } else {
        if (seenIDs.has(d.ID)) return; // already handled
        if (indexer === -1) return; // need parent node for normal messages

        const childNode = new Node(0, {
          message: {},
          x: nodes[indexer].cap.x + (Math.random() - 0.5),
          y: nodes[indexer].cap.y + 4
        });
        childNode.ID = nodes[indexer].ID + (d.usercolor || "") + "." + (childSearch(nodes[indexer].children, d.usercolor)+ 1);
        seenIDs.add(childNode.ID);
        childNode.usercolor = d.usercolor;
        childNode.content.message = d.audio || null;
        childNode.messageType = "text";
        nodes[indexer].children.push(childNode);
        nodes.push(childNode);
        startmouse = 50;
        overlap = 50;
      }

    // ---------- AUDIO PACKETS (ArrayBuffer) ----------
    } else if (data instanceof ArrayBuffer) {
      // console.log("Processing ArrayBuffer of size:", data.byteLength);
      
    //   // Debug: Let's see what's act
    
      
      // Rest of ArrayBuffer processing for larger messages
      if (data.byteLength < 4) {
        // console.warn("ArrayBuffer too small for metadata length");
        return;
      }
      
      const view = new DataView(data);
      const metadataLength = view.getUint32(0, true);
      
      // Validate metadata length is reasonable and within bounds
      if (metadataLength < 0 || metadataLength > data.byteLength - 4 || metadataLength > 10000) {
        // console.log("Skipping ArrayBuffer with invalid metadata length:", metadataLength);
        return;
      }
      
      // Safe construction of metadata bytes
      let metadataBytes;
      try {
        metadataBytes = new Uint8Array(data, 4, metadataLength);
      } catch (e) {
        // console.warn("Failed to create metadata bytes:", e);
        return;
      }
      
      let metadata;
      try {
        metadata = JSON.parse(new TextDecoder().decode(metadataBytes));
      } catch (err) {
        console.warn("failed to parse metadata from audio packet", err);
        return;
      }

      // safe slice for audio bytes - validate bounds first
      const audioStartIndex = 4 + metadataLength;
      if (audioStartIndex >= data.byteLength) {
        console.warn("No audio data in packet");
        return;
      }
      
      const audioBytes = data.slice(audioStartIndex);
      if (audioBytes.byteLength === 0) {
        console.warn("Empty audio data");
        return;
      }
      const audioBlob = new Blob([audioBytes], { type: "audio/webm" });
      const url = URL.createObjectURL(audioBlob);
      const audio = new Audio();
      audio.src = url;

      // Use promise-based loading to avoid blocking
      await new Promise((resolve) => {
        audio.addEventListener("error", (e) => {
          console.error("Audio loading error:", e);
          URL.revokeObjectURL(url);
          resolve();
        });
        audio.addEventListener("loadeddata", () => {
          resolve();
        });
        // Fallback timeout
        setTimeout(resolve, 1000);
      });

      // RESEND branch
      if (metadata.resend === 1) {
        if (seenIDs.has(metadata.ID)) return; // skip if already seen

        const numbeee = typeof trimVersion === 'function' ? trimVersion(metadata.ID) : metadata.ID;

        let indexer = nodes.findIndex(n => n && (n.ID == numbeee || n.ID == metadata.ID));
        if (indexer === -1) {
          if (typeof numbeee === 'number') {
            const node = new Node(0, {
              message: {},
              x: metadata.x,
              y: metadata.y
            });
            const high = new Rectangle(metadata.x - (metadata.width / 2), metadata.y - rect1.height, metadata.width, rect1.height, metadata.usercolor + '60');
            highs.push(high);
            node.ID = metadata.ID;
            seenIDs.add(node.ID);
            node.width = metadata.width;
            node.usercolor = metadata.usercolor;
            node.content.message = audio;
            allaud.push(node.content.message);
            topnodes.push(node);
            nodes.push(node);
            startmouse = 50;
            overlap = 50;
          }
          return;
        }

        const childNode = new Node(0, {
          message: {},
          x: nodes[indexer].cap.x + (Math.random() - 0.5),
          y: nodes[indexer].cap.y + 4
        });
        childNode.ID = metadata.ID;
        seenIDs.add(childNode.ID);
        childNode.usercolor = metadata.usercolor;
        childNode.content.message = audio;
        allaud.push(childNode.content.message);
        nodes[indexer].children.push(childNode);
        nodes.push(childNode);
        startmouse = 50;
        overlap = 50;

      // NORMAL audio branch
      } else {
        if (seenIDs.has(metadata.ID)) return;
        let indexer = nodes.findIndex(n => n && n.ID == metadata.ID);
        if (indexer === -1) return;
        
        const childNode = new Node(0, {
          message: {},
          x: nodes[indexer].cap.x + (Math.random() - 0.5),
          y: nodes[indexer].cap.y + 4
        });
        childNode.ID = nodes[indexer].ID + (metadata.usercolor || "") + "." + (childSearch(nodes[indexer].children, metadata.usercolor) + 1);
        seenIDs.add(childNode.ID);
        childNode.usercolor = metadata.usercolor;
        childNode.content.message = audio;
        allaud.push(childNode.content.message);
        nodes[indexer].children.push(childNode);
        nodes.push(childNode);
        startmouse = 50;
        overlap = 50;
      }
    }
  } catch (e) {
    console.error("Failed to process message:", e);
  }

  // Trim seenIDs if it gets too large (moved to separate function)
  trimSeenIds();
}




function trimSeenIds() {
  try {
    const MAX_SEEN = 50000;
    if (seenIDs.size > MAX_SEEN) {
      const toEvict = 10000;
      const arr = Array.from(seenIDs);
      for (let i = 0; i < toEvict && arr[i]; i++) {
        seenIDs.delete(arr[i]);
      }
    }
  } catch (e) {
    console.warn("seenIDs trim error", e);
  }
}



const originalProcessSignaling = processSignalingMessage;
processSignalingMessage = async function(msg) {
    await originalProcessSignaling(msg);
    exposeToGlobal();
};
setTimeout(() => {
    exposeToGlobal();
    // console.log("Variables exposed to global scope");
}, 1000);

// optional: when underlying raw socket opens, attach things that depended on ws.open
// Quick message handler - just queue the work
window.onRobustMessage = async (event, rawSocket) => {
  let msg = null;

  // Try to parse the incoming message
  if (typeof event.data === "string") {
    try {
      msg = JSON.parse(event.data);
      // console.log("Received JSON string:", msg);
    } catch (err) {
      // console.warn("Failed to parse string message:", err, event.data);
      msg = null;
    }
  } else if (event.data instanceof ArrayBuffer) {
    try {
      const text = new TextDecoder().decode(event.data);
      msg = JSON.parse(text);
      // console.log("Received JSON ArrayBuffer:", msg);
    } catch (err) {
      // console.log("Binary message (not JSON), passing through:", err);
      msg = null;
    }
  }

  // --- Handle special resendList messages immediately ---
  if (msg && msg[0] === "resendList" && msg[1] === "please") {
    // console.log("Processing resendList request");
    try {
      // Disable sending for matching node IDs
      for (let k = 0; k < nodes.length; k++) {
        if (msg.includes(nodes[k].ID)) {
          nodes[k].sending = 0;
        }
      }
    } catch (e) {
      console.error("Error processing resendList:", e);
    }
    return;
  }

  // --- Handle WebRTC signaling (offer/answer/candidate) ---
  if (msg && (msg.offer || msg.answer || msg.candidate)) {
    // console.log("Processing WebRTC signaling:", Object.keys(msg));

    if (!window.webrtcInitialized) {
      // console.log("WebRTC not ready ‚Äî queueing signaling message");
      if (!window.pendingSignalingMessages) window.pendingSignalingMessages = [];
      window.pendingSignalingMessages.push(msg);
      return;
    }

    try {
      await window.processSignalingMessage(msg);
    } catch (error) {
      console.error("WebRTC signaling error:", error);
    }
    return;
  }

  // --- Queue all other messages for batch processing ---
  if (!window.messageQueue) window.messageQueue = [];
  const workItem = {
    event,
    rawSocket,
    timestamp: Date.now(),
    data: event.data,
  };
  window.messageQueue.push(workItem);

  if (!window.processingMessages) {
    window.scheduleMessageProcessing();
  }
};




// Add this at the end of your module script (inside <script type="module">)
// This exposes your module variables to the global scope for debugging

// Expose WebRTC variables
window.pc = pc;
window.signaling = signaling;
window.webrtcInitialized = webrtcInitialized;
window.isPolite = isPolite;
window.makingOffer = makingOffer;
window.ignoreOffer = ignoreOffer;
window.pendingIceCandidates = pendingIceCandidates;
window.robust = robust;

// Expose audio variables  
window.globalAudioContext = globalAudioContext;
window.audioContextReady = audioContextReady;

// Update these whenever they change
window.updateGlobals = function() {
    window.pc = pc;
    window.signaling = signaling;
    window.webrtcInitialized = webrtcInitialized;
    window.isPolite = isPolite;
    window.makingOffer = makingOffer;
    window.ignoreOffer = ignoreOffer;
    window.globalAudioContext = globalAudioContext;
    window.audioContextReady = audioContextReady;
};

// Call updateGlobals whenever WebRTC state changes

// Also update after signaling
const originalProcessSignalingMessage = processSignalingMessage;
window.processSignalingMessage = async function(msg) {
    await originalProcessSignalingMessage(msg);
    window.updateGlobals();
};

function armResumeAudioContext() {
  const ctx = Tone.getContext().rawContext;
  if (ctx.state === "suspended") {
    const resume = () => ctx.resume().then(() => {
      // console.log("AudioContext resumed by user gesture");
    });
    ["click", "keydown", "touchstart", "pointerdown"].forEach(evt => {
      document.addEventListener(evt, resume, { once: true });
    });
  }
}
armResumeAudioContext();



    </script>
</html>
